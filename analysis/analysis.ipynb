{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d151dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import pdist\n",
    "from scipy.stats import spearmanr, rankdata, entropy\n",
    "from scipy import stats\n",
    "from typing import List, Dict, Any, Optional\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7722cda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _set_global_seed(seed: int):\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.use_deterministic_algorithms(True)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "_set_global_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a5fcfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_latest_interaction_file(\n",
    "    logs_root: str,\n",
    "    seed_folder: str,\n",
    "    split: str = \"validation\",\n",
    "    prefix: str = \"interaction_gpu0\"\n",
    ") -> object:\n",
    "    pattern = os.path.join(\n",
    "        logs_root,\n",
    "        seed_folder,\n",
    "        \"interactions\",\n",
    "        split,\n",
    "        \"epoch_*\",\n",
    "        f\"{prefix}*\"\n",
    "    )\n",
    "    files = glob.glob(pattern)\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No files matching {pattern}\")\n",
    "\n",
    "    # parse epoch number from folder name\n",
    "    def parse_epoch(path: str) -> int:\n",
    "        folder = os.path.basename(os.path.dirname(path))\n",
    "        return int(folder.split(\"_\", 1)[1])\n",
    "\n",
    "    # sort files by epoch and select the last one\n",
    "    sorted_files = sorted(files, key=parse_epoch)\n",
    "    last_file = sorted_files[-1]\n",
    "    print(last_file)\n",
    "\n",
    "    return torch.load(last_file, map_location=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d67653",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_interaction_file(\n",
    "    logs_root: str,\n",
    "    seed_folder: str,\n",
    "    epoch: int,\n",
    "    split: str = \"validation\",\n",
    "    prefix: str = \"interaction_gpu0\"\n",
    ") -> object:\n",
    "    \"\"\"\n",
    "    Load the interaction file for a specified epoch.\n",
    "    \"\"\"\n",
    "    epoch_folder = f\"epoch_{epoch}\"\n",
    "    pattern = os.path.join(\n",
    "        logs_root,\n",
    "        seed_folder,\n",
    "        \"interactions\",\n",
    "        split,\n",
    "        epoch_folder,\n",
    "        f\"{prefix}*\"\n",
    "    )\n",
    "\n",
    "    files = glob.glob(pattern)\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No files matching {pattern}\")\n",
    "\n",
    "    if len(files) > 1:\n",
    "        # If multiple files match, sort alphabetically and pick the first\n",
    "        files = sorted(files)\n",
    "\n",
    "    selected_file = files[0]\n",
    "    print(f\"Loading interaction file: {selected_file}\")\n",
    "\n",
    "    return torch.load(selected_file, map_location=\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2743e59f",
   "metadata": {},
   "source": [
    "# Bee Language Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267e494b",
   "metadata": {},
   "source": [
    "## Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9a31b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "bee_interaction_42 = load_interaction_file(\n",
    "    logs_root=\"../logs/interactions/2025-07-02\",\n",
    "    seed_folder=\"gamesize10_bee_gs_seed42\",\n",
    "    epoch=26,\n",
    "    split=\"validation\",\n",
    "    prefix=\"interaction_gpu0\"\n",
    ")\n",
    "bee_interaction_27 = load_interaction_file(\n",
    "    logs_root=\"../logs/interactions/2025-07-02\",\n",
    "    seed_folder=\"gamesize10_bee_gs_seed27\",\n",
    "    epoch=30,\n",
    "    split=\"validation\",\n",
    "    prefix=\"interaction_gpu0\"\n",
    ")\n",
    "bee_interaction_31 = load_interaction_file(\n",
    "    logs_root=\"../logs/interactions/2025-07-02\",\n",
    "    seed_folder=\"gamesize10_bee_gs_seed31\",\n",
    "    epoch=40,\n",
    "    split=\"validation\",\n",
    "    prefix=\"interaction_gpu0\"\n",
    ")\n",
    "bee_interaction_2025 = load_interaction_file(\n",
    "    logs_root=\"../logs/interactions/2025-07-02\",\n",
    "    seed_folder=\"gamesize10_bee_gs_seed2025\",\n",
    "    epoch=26,\n",
    "    split=\"validation\",\n",
    "    prefix=\"interaction_gpu0\"\n",
    ")\n",
    "bee_interaction_123 = load_interaction_file(\n",
    "    logs_root=\"../logs/interactions/2025-07-02\",\n",
    "    seed_folder=\"gamesize10_bee_gs_seed123\",\n",
    "    epoch=14,\n",
    "    split=\"validation\",\n",
    "    prefix=\"interaction_gpu0\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c667e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "bee_interactions = [\n",
    "    (\"seed42\", bee_interaction_42),\n",
    "    (\"seed27\", bee_interaction_27),\n",
    "    (\"seed31\", bee_interaction_31),\n",
    "    (\"seed123\", bee_interaction_123),\n",
    "    (\"seed2025\", bee_interaction_2025),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c287c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "plt.rcParams.update({\n",
    "    \"font.size\": 16,\n",
    "    \"axes.titlesize\": 16,\n",
    "    \"axes.labelsize\": 16,\n",
    "    \"xtick.labelsize\": 16,\n",
    "    \"ytick.labelsize\": 16,\n",
    "    \"legend.fontsize\": 16,\n",
    "})\n",
    "\n",
    "def extract_distances(interaction_obj):\n",
    "    message = interaction_obj.message\n",
    "    distance_token = message[:, 8].cpu().numpy()\n",
    "    semantic_distances = []\n",
    "    graph_counter = 0\n",
    "    for data_batch in interaction_obj.aux_input['data']:\n",
    "        graphs_in_batch = data_batch.ptr.size(0) - 1\n",
    "        for j in range(graphs_in_batch):\n",
    "            nest_global = int(interaction_obj.aux_input['nest_idx'][graph_counter + j])\n",
    "            food_global = int(interaction_obj.aux_input['food_idx'][graph_counter + j])\n",
    "            start_index  = int(data_batch.ptr[j])\n",
    "            nest_local   = nest_global - start_index\n",
    "            food_local   = food_global - start_index\n",
    "            dx, dy = (data_batch.pos[food_local] - data_batch.pos[nest_local]).cpu().numpy()\n",
    "            semantic_distances.append(np.hypot(dx, dy))\n",
    "        graph_counter += graphs_in_batch\n",
    "    true_distances = np.array(semantic_distances)\n",
    "    return true_distances, distance_token\n",
    "\n",
    "def extract_directions(interaction_obj):\n",
    "    message = interaction_obj.message\n",
    "    predicted_direction = message[:, :8].argmax(dim=1).cpu().numpy()\n",
    "    semantic_dirs = []\n",
    "    graph_counter = 0\n",
    "    for data_batch in interaction_obj.aux_input['data']:\n",
    "        graphs_in_batch = data_batch.ptr.size(0) - 1\n",
    "        for j in range(graphs_in_batch):\n",
    "            nest_global = int(interaction_obj.aux_input['nest_idx'][graph_counter + j])\n",
    "            food_global = int(interaction_obj.aux_input['food_idx'][graph_counter + j])\n",
    "            start_index  = int(data_batch.ptr[j])\n",
    "            nest_local   = nest_global - start_index\n",
    "            food_local   = food_global - start_index\n",
    "            dx, dy = (data_batch.pos[food_local] - data_batch.pos[nest_local]).cpu().numpy()\n",
    "            angle_deg = (math.degrees(math.atan2(dy, dx)) + 360) % 360\n",
    "            dir_id    = int((angle_deg + 22.5) // 45) % 8\n",
    "            semantic_dirs.append(dir_id)\n",
    "        graph_counter += graphs_in_batch\n",
    "    true_directions = np.array(semantic_dirs)\n",
    "    return true_directions, predicted_direction\n",
    "\n",
    "interactions = [\n",
    "    (\"seed42\", bee_interaction_42),\n",
    "    (\"seed27\", bee_interaction_27),\n",
    "    (\"seed31\",  bee_interaction_31),\n",
    "    (\"seed123\", bee_interaction_123),\n",
    "    (\"seed2025\", bee_interaction_2025),\n",
    "]\n",
    "\n",
    "num_seeds = len(interactions)\n",
    "fig, axes = plt.subplots(nrows=num_seeds, ncols=2, figsize=(12, 4*num_seeds))\n",
    "\n",
    "for idx, (label, interaction_obj) in enumerate(interactions):\n",
    "    # Distance subplot\n",
    "    true_distances, distance_token = extract_distances(interaction_obj)\n",
    "    true_log = true_distances\n",
    "    token_log = distance_token\n",
    "    rho, _ = spearmanr(true_log, token_log)\n",
    "    ax_dist = axes[idx, 0]\n",
    "    ax_dist.scatter(true_log, token_log, alpha=0.3)\n",
    "    ax_dist.set_xlabel(\"True Distance\", fontsize=16)\n",
    "    ax_dist.set_ylabel(\"Distance Token\", fontsize=16)\n",
    "    ax_dist.set_title(f\"{label} (œÅ={rho:.2f})\", fontsize=16)\n",
    "\n",
    "    # Direction subplot\n",
    "    true_dirs, pred_dirs = extract_directions(interaction_obj)\n",
    "    jitter = 0.1\n",
    "    x = true_dirs + np.random.uniform(-jitter, jitter, size=true_dirs.shape)\n",
    "    y = pred_dirs + np.random.uniform(-jitter, jitter, size=pred_dirs.shape)\n",
    "    ax_dir = axes[idx, 1]\n",
    "    ax_dir.scatter(x, y, alpha=0.3)\n",
    "    ax_dir.set_xlabel(\"True Direction ID\", fontsize=16)\n",
    "    ax_dir.set_ylabel(\"Predicted Direction ID\", fontsize=16)\n",
    "    ax_dir.set_xticks(range(8))\n",
    "    ax_dir.set_yticks(range(8))\n",
    "    ax_dir.set_title(f\"{label}\", fontsize=16)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbfe9bd",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ded4202",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_true_dirs, confusion_pred_dirs = extract_directions(bee_interaction_42)\n",
    "accuracy = (confusion_pred_dirs  == confusion_true_dirs).mean()\n",
    "print(f\"Direction accuracy = {accuracy*100:.1f}%\")\n",
    "\n",
    "# or confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(confusion_true_dirs, confusion_pred_dirs , labels=range(8))\n",
    "cm_norm = cm.astype(float) / cm.sum(axis=1, keepdims=True) * 100\n",
    "print(\"Raw counts:\\n\", cm)\n",
    "print(\"\\nRow‚Äënormalized (%):\\n\", np.round(cm_norm, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7adaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"N\",\"NE\",\"E\",\"SE\",\"S\",\"SW\",\"W\",\"NW\"]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "im = ax.imshow(cm_norm, interpolation='nearest', cmap='Blues')\n",
    "\n",
    "ax.set_xticks(np.arange(len(labels)))\n",
    "ax.set_xticklabels(labels, rotation=45, ha=\"right\", fontsize=12)\n",
    "ax.set_yticks(np.arange(len(labels)))\n",
    "ax.set_yticklabels(labels, fontsize=12)\n",
    "\n",
    "for i in range(cm_norm.shape[0]):\n",
    "    for j in range(cm_norm.shape[1]):\n",
    "        text_color = \"white\" if cm[i, j] > 50 else \"black\"\n",
    "        ax.text(j, i, f\"{cm_norm[i, j]:.1f}%\", ha=\"center\", va=\"center\",\n",
    "                color=text_color, fontsize=10)\n",
    "\n",
    "cbar = fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "cbar.ax.tick_params(labelsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f631eaa8",
   "metadata": {},
   "source": [
    "## Token Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c787045",
   "metadata": {},
   "outputs": [],
   "source": [
    "message = interaction_obj.message\n",
    "distance_token = message[:, 8].cpu().numpy()\n",
    "direction_token = message[:, :8].argmax(dim=1).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a962c7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "IDX_TO_DIR = {0: \"N\", 1: \"NE\", 2: \"E\", 3: \"SE\",\n",
    "              4: \"S\", 5: \"SW\", 6: \"W\", 7: \"NW\"}\n",
    "\n",
    "unique, counts = np.unique(direction_token, return_counts=True)\n",
    "labels = [IDX_TO_DIR[idx] for idx in unique]\n",
    "\n",
    "plt.figure()\n",
    "plt.bar(labels, counts)\n",
    "plt.xlabel(\"Direction\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Direction Token Distribution\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(distance_token, bins=10, rwidth=0.8)\n",
    "plt.xlabel(\"Distance Token Value\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Distance Token Distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba37f1cb",
   "metadata": {},
   "source": [
    "## Compositionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2c33bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIRECTIONS = {\n",
    "    \"N\": 0, \"NE\": 1, \"E\": 2, \"SE\": 3,\n",
    "    \"S\": 4, \"SW\": 5, \"W\": 6, \"NW\": 7\n",
    "}\n",
    "# inverse mapping for integer codes back to strings\n",
    "INV_DIRECTIONS: Dict[int, str] = {v: k for k, v in DIRECTIONS.items()}\n",
    "\n",
    "SECTOR_ANGLE = 2 * np.pi / len(DIRECTIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959c59f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph(batch) -> nx.DiGraph:\n",
    "    \"\"\"\n",
    "    Construct a directed graph from a DataBatch\n",
    "    \"\"\"\n",
    "    edge_indices = batch.edge_index.cpu().numpy().T\n",
    "    edge_attrs = batch.edge_attr.cpu().numpy()\n",
    "\n",
    "    graph = nx.DiGraph()\n",
    "    for (u, v), (dist, dir_raw) in zip(edge_indices, edge_attrs):\n",
    "        ui, vi = int(u), int(v)\n",
    "        if isinstance(dir_raw, (bytes, str)):\n",
    "            dir_str = dir_raw.decode() if isinstance(dir_raw, bytes) else dir_raw\n",
    "            if dir_str not in DIRECTIONS:\n",
    "                raise ValueError(f\"Unknown direction '{dir_str}' in edge_attrs\")\n",
    "        else:\n",
    "            dir_int = int(dir_raw)\n",
    "            dir_str = INV_DIRECTIONS.get(dir_int)\n",
    "            if dir_str is None:\n",
    "                raise ValueError(f\"Unknown direction code {dir_int} in edge_attrs\")\n",
    "        graph.add_edge(ui, vi, distance=float(dist), direction=dir_str)\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b930b59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the build graph function on 1 graph\n",
    "first_batch = bee_interaction_42.aux_input[\"data\"][0]\n",
    "graphs = first_batch.to_data_list()  \n",
    "single = graphs[0]\n",
    "G = build_graph(single)\n",
    "print(f\"Graph has {G.number_of_nodes()} nodes and {G.number_of_edges()} edges\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e4416d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for u, v, attrs in list(G.edges(data=True))[:5]:\n",
    "    print(f\"  {u}->{v} dist={attrs['distance']:.2f}, dir={attrs['direction']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f996dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_single_graph(batch):\n",
    "    G = build_graph(batch)\n",
    "\n",
    "    pos = {i: tuple(batch.pos[i].cpu().numpy()) for i in range(batch.pos.size(0))}\n",
    "\n",
    "    # x[:,0]=nest, x[:,1]=food, x[:,2]=distractor\n",
    "    types = {}\n",
    "    for i, feat in enumerate(batch.x.cpu().numpy()):\n",
    "        idx = feat.argmax()\n",
    "        types[i] = \"nest\" if idx == 0 else \"food\" if idx == 1 else \"distractor\"\n",
    "\n",
    "    color_map = {\"nest\": \"lightblue\", \"food\": \"red\", \"distractor\": \"grey\"}\n",
    "    node_colors = [color_map[types[i]] for i in G.nodes()]\n",
    "    labels = {i: types[i] for i in G.nodes()}\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    nx.draw_networkx_nodes(G, pos, node_color=node_colors, node_size=500)\n",
    "    nx.draw_networkx_labels(G, pos, labels, font_size=8)\n",
    "\n",
    "    for u, v, d in G.edges(data=True):\n",
    "        nx.draw_networkx_edges(\n",
    "            G,\n",
    "            pos,\n",
    "            edgelist=[(u, v)],\n",
    "            arrowstyle='-|>',\n",
    "            arrowsize=12,\n",
    "            connectionstyle='arc3,rad=0.1'\n",
    "        )\n",
    "        edge_label = { (u, v): f\"{d['distance']:.1f}m, dir={d['direction']}\" }\n",
    "        nx.draw_networkx_edge_labels(\n",
    "            G,\n",
    "            pos,\n",
    "            edge_labels=edge_label,\n",
    "            font_size=7,\n",
    "            label_pos=0.4\n",
    "        )\n",
    "\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "visualize_single_graph(single)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b47780",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_meaning_spaces(\n",
    "    interaction,\n",
    "    hypothesis: str\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Generate meaning vectors [distance, direction] under specified hypotheses.\n",
    "\n",
    "    Always assumes shortest path and uses only edge attributes (except for coordinates).\n",
    "    \"\"\"\n",
    "    # split batched graphs into individual graph\n",
    "    data_list = []\n",
    "    for batch in interaction.aux_input['data']:\n",
    "        data_list.extend(batch.to_data_list())\n",
    "\n",
    "    nest_indices = interaction.aux_input['nest_idx'].tolist()\n",
    "    food_indices = interaction.aux_input['food_idx'].tolist()\n",
    "    if len(data_list) != len(nest_indices):\n",
    "        raise ValueError(\n",
    "            f\"Mismatch examples vs indices: {len(data_list)} graphs but {len(nest_indices)} nest indices\"\n",
    "        )\n",
    "\n",
    "    representations: List[List[float]] = []\n",
    "\n",
    "    \n",
    "\n",
    "    for i, data in enumerate(data_list):\n",
    "        # positions and graph\n",
    "        pos = data.pos.cpu().numpy()\n",
    "        G   = build_graph(data)\n",
    "\n",
    "        # local start/target\n",
    "        start_idx  = nest_indices[i]\n",
    "        target_idx = food_indices[i]\n",
    "        p0, p1     = pos[start_idx], pos[target_idx]\n",
    "\n",
    "        # shortest-path\n",
    "        hop_paths = list(nx.all_shortest_paths(G, source=start_idx, target=target_idx)) \n",
    "        def total_dist(path):\n",
    "            return sum(G[u][v]['distance'] for u, v in zip(path, path[1:]))\n",
    "        path_info = [(p, total_dist(p)) for p in hop_paths]\n",
    "        min_d = min(d for _, d in path_info)\n",
    "        best_paths = [p for p, d in path_info if d == min_d]\n",
    "        best_path = min(best_paths)\n",
    "        edges     = list(zip(best_path, best_path[1:]))\n",
    "\n",
    "        DIRECTION_TO_DEGREES_VOCAB: Dict[str, float] = {\n",
    "        \"N\": 90.0, \"NE\": 45.0, \"E\": 0.0, \"SE\": 315.0,\n",
    "        \"S\": 270.0, \"SW\": 225.0, \"W\": 180.0, \"NW\": 135.0\n",
    "    }\n",
    "        COMPASS_VECS: Dict[str, np.ndarray] = {\n",
    "            d: np.array([np.cos(np.deg2rad(phi)), np.sin(np.deg2rad(phi))])\n",
    "            for d, phi in DIRECTION_TO_DEGREES_VOCAB.items()\n",
    "        }\n",
    "        SECTOR_ANGLE = 2 * np.pi / 8\n",
    "\n",
    "        def degrees_to_discrete_direction(degrees):\n",
    "            \"\"\"Convert degrees to discrete direction using binning\"\"\"\n",
    "            # Normalize to 0-360\n",
    "            degrees = degrees % 360.0\n",
    "            # Bin into 8 sectors (each 45 degrees wide)\n",
    "            sector_idx = int((degrees + 22.5) // 45) % 8\n",
    "            # Map sector index to direction\n",
    "            sectors = [\"E\", \"NE\", \"N\", \"NW\", \"W\", \"SW\", \"S\", \"SE\"]\n",
    "            return sectors[sector_idx]\n",
    "\n",
    "        # prepare edge attributes\n",
    "        edge_dirs = [G[u][v]['direction'] for u, v in edges]\n",
    "        edge_degs = [DIRECTION_TO_DEGREES_VOCAB[dir_str] for dir_str in edge_dirs]\n",
    "        edge_vecs = [COMPASS_VECS[dir_str] for dir_str in edge_dirs]\n",
    "        edge_dists= [G[u][v]['distance'] for u, v in edges]\n",
    "\n",
    "        total_dist = float(sum(edge_dists))\n",
    "        hop_count  = float(len(edges))\n",
    "\n",
    "        # straight-line metrics\n",
    "        dx, dy      = p1[0] - p0[0], p1[1] - p0[1]\n",
    "        straight_dist = float(np.hypot(dx, dy))\n",
    "        straight_ang  = float(np.arctan2(dy, dx))\n",
    "\n",
    "        # angle arithmetic: sum of degrees\n",
    "        # treats directions like rotations (turning left/right)\n",
    "        sum_deg = sum(edge_degs) % 360.0\n",
    "        # map to sector index\n",
    "        sector_idx = int((sum_deg + 22.5) // 45) % 8\n",
    "        angle_arith = sector_idx * SECTOR_ANGLE\n",
    "\n",
    "        # vector sum compass\n",
    "        # treats distance like displacements (moving through space)\n",
    "        vsum = np.sum(edge_vecs, axis=0) if edge_vecs else np.array([0.0, 0.0])\n",
    "        if np.allclose(vsum, 0):\n",
    "            # fallback to first direction\n",
    "            sector_idx_vs = list(DIRECTION_TO_DEGREES_VOCAB.keys()).index(edge_dirs[0]) if edge_dirs else 0\n",
    "        else:\n",
    "            ang_v_deg = np.degrees(np.arctan2(vsum[1], vsum[0])) % 360.0\n",
    "            sector_idx_vs = int((ang_v_deg + 22.5) // 45) % 8\n",
    "        angle_vs = sector_idx_vs * SECTOR_ANGLE\n",
    "\n",
    "        if hypothesis == 'coordinates':\n",
    "            distance, direction = straight_dist, straight_ang\n",
    "\n",
    "        elif hypothesis == 'hop_count_distance_vector_sum_direction':\n",
    "            distance, direction = hop_count, angle_vs\n",
    "\n",
    "        elif hypothesis == 'sum_distances_vector_sum_direction':\n",
    "            distance, direction = total_dist, angle_vs\n",
    "\n",
    "        elif hypothesis == 'hop_count_distance_angle_direction':\n",
    "            distance, direction = hop_count, angle_arith\n",
    "\n",
    "        elif hypothesis == 'sum_distances_angle_direction':\n",
    "            distance, direction = total_dist, angle_arith\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown hypothesis {hypothesis}\")\n",
    "\n",
    "        representations.append([distance, direction])\n",
    "\n",
    "    return np.array(representations, dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce35e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_topsim(\n",
    "    message_vectors: np.ndarray,\n",
    "    meaning_vectors: np.ndarray,\n",
    "    normalize: str = \"linear\"\n",
    ") -> float:\n",
    "    # 1) custom message‚Äêspace metric on [dir_token, distance]\n",
    "    def message_metric(u, v):\n",
    "        tok_u, ru = int(u[0]) % 8, u[1]\n",
    "        tok_v, rv = int(v[0]) % 8, v[1]\n",
    "        Œ∏u = 2*np.pi * tok_u / 8\n",
    "        Œ∏v = 2*np.pi * tok_v / 8\n",
    "\n",
    "        dr = ru - rv\n",
    "        ŒîŒ∏ = abs(Œ∏u - Œ∏v) % (2*np.pi)\n",
    "        if ŒîŒ∏ > np.pi:\n",
    "            ŒîŒ∏ = 2*np.pi - ŒîŒ∏\n",
    "        ŒîŒ∏_norm = ŒîŒ∏ / np.pi\n",
    "\n",
    "        return np.hypot(dr, ŒîŒ∏_norm)\n",
    "\n",
    "    # 2) message distances\n",
    "    msg_dists = pdist(message_vectors, metric=message_metric)\n",
    "    if np.std(msg_dists) == 0:\n",
    "        return np.nan\n",
    "\n",
    "    # 3) extract raw dist & angle from meanings\n",
    "    d = meaning_vectors[:, 0]\n",
    "    a = meaning_vectors[:, 1]\n",
    "    n = len(d)\n",
    "\n",
    "    # 4) normalize the distance column\n",
    "    if normalize == \"linear\":\n",
    "        if d.max() != d.min():\n",
    "            d_norm = (d - d.min()) / (d.max() - d.min())\n",
    "        else:\n",
    "            d_norm = np.zeros_like(d)\n",
    "    elif normalize == \"log\":\n",
    "        d_log = np.log1p(d)\n",
    "        if d_log.max() != d_log.min():\n",
    "            d_norm = (d_log - d_log.min()) / (d_log.max() - d_log.min())\n",
    "        else:\n",
    "            d_norm = np.zeros_like(d_log)\n",
    "    elif normalize == \"rank\":\n",
    "        ranks = rankdata(d, method=\"average\")\n",
    "        d_norm = (ranks - 1) / (n - 1)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown normalize={normalize!r}\")\n",
    "\n",
    "    # 5) build meaning‚Äêspace distances\n",
    "    meaning_dists = []\n",
    "    for i in range(n - 1):\n",
    "        for j in range(i + 1, n):\n",
    "            dd = d_norm[i] - d_norm[j]\n",
    "            ŒîŒ∏ = abs(a[i] - a[j]) % (2 * np.pi)\n",
    "            if ŒîŒ∏ > np.pi:\n",
    "                ŒîŒ∏ = 2*np.pi - ŒîŒ∏\n",
    "            Œ∏_norm = ŒîŒ∏ / np.pi\n",
    "            meaning_dists.append(np.hypot(dd, Œ∏_norm))\n",
    "\n",
    "    meaning_dists = np.array(meaning_dists)\n",
    "    if np.std(meaning_dists) == 0:\n",
    "        return np.nan\n",
    "\n",
    "    # 6) Spearman‚ÄêœÅ\n",
    "    return spearmanr(msg_dists, meaning_dists, nan_policy=\"raise\").correlation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22f2562",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mutual_info_score\n",
    "\n",
    "def _percentile_bins(x: np.ndarray, n_bins: int) -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Return digitised array and the bin edges used.\"\"\"\n",
    "    pct  = np.linspace(0, 100, n_bins + 1)\n",
    "    edges = np.percentile(x, pct)\n",
    "    edges[-1] += 1e-12\n",
    "    return np.digitize(x, edges) - 1, edges\n",
    "\n",
    "def _shannon_entropy(ids: np.ndarray) -> float:\n",
    "    vals, cnts = np.unique(ids, return_counts=True)\n",
    "    p = cnts / cnts.sum()\n",
    "    return float(-np.sum(p * np.log(p + 1e-12)))\n",
    "\n",
    "def _angle_to_sector(theta: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"8-way compass bin  (E=0, NE=1, ‚Ä¶).\"\"\"\n",
    "    return (((theta % (2*np.pi)) + np.pi/8) // (np.pi/4)).astype(int)\n",
    "\n",
    "def compute_posdis(\n",
    "    meanings : np.ndarray, # (N,2)  [dist , Œ∏(rad)]\n",
    "    messages : np.ndarray, # (N,2)  [dist_tok , dir_tok]\n",
    "    *, n_bins_distance: int = 2\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Positional disentanglement (Chaabouni et al.2020).\n",
    "    Returns a value in [0,1]; higher means cleaner token-concept alignment.\n",
    "    \"\"\"\n",
    "    # discretise concepts\n",
    "    concept_dist_disc, dist_edges = _percentile_bins(meanings[:, 0], n_bins_distance)\n",
    "    concept_dir_disc  = _angle_to_sector(meanings[:, 1])\n",
    "    concepts          = np.column_stack([concept_dist_disc, concept_dir_disc])\n",
    "\n",
    "    pos_scores = []\n",
    "\n",
    "    for pos in (0, 1):\n",
    "        token_raw = messages[:, pos]\n",
    "\n",
    "        # discretise token if continuous (distance position)\n",
    "        if pos == 0:\n",
    "            token_disc, _ = _percentile_bins(token_raw, n_bins_distance)\n",
    "        else:\n",
    "            token_disc   = token_raw.astype(int)\n",
    "\n",
    "        # mutual information with each concept\n",
    "        mi = [mutual_info_score(token_disc, concepts[:, i]) for i in (0, 1)]\n",
    "        mi_sorted = sorted(mi, reverse=True)\n",
    "        top1, top2 = mi_sorted + [0.0] * (2 - len(mi_sorted))   # pad if less than 2\n",
    "\n",
    "        H = _shannon_entropy(token_disc)\n",
    "        score = (top1 - top2) / H if H > 0 else 0.0\n",
    "        pos_scores.append(score)\n",
    "\n",
    "    total = float(np.mean(pos_scores))\n",
    "    breakdown = {'distance_token' : pos_scores[0],\n",
    "                'direction_token': pos_scores[1]}\n",
    "\n",
    "    return total, breakdown\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2877f45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "\n",
    "class _TREModel(nn.Module):\n",
    "    \"\"\"Linear-composition model  s = A œÜ_d + B œÜ_dir  (Appendix C of the paper).\"\"\"\n",
    "    def __init__(self, vocab_size: int, emb: int = 32):\n",
    "        super().__init__()\n",
    "        self.dist_emb = nn.Linear(1, emb)        # œÜ(distance_token)\n",
    "        self.dir_emb  = nn.Embedding(vocab_size, emb)\n",
    "        self.A = nn.Linear(emb, 3, bias=False)   # -> [z_dist , z_cos , z_sin]\n",
    "        self.B = nn.Linear(emb, 3, bias=False)\n",
    "\n",
    "    def forward(self, dist_t: torch.Tensor, dir_t: torch.Tensor) -> torch.Tensor:\n",
    "        z = self.A(self.dist_emb(dist_t)) + self.B(self.dir_emb(dir_t))\n",
    "        return z \n",
    "\n",
    "def _loss_tre(pred: torch.Tensor, target: torch.Tensor, \n",
    "              dist_weight: float = 1.0, dir_weight: float = 1.0) -> torch.Tensor:\n",
    "    \"\"\"MSE on distance + cos/sin of angle with separate weights.\"\"\"\n",
    "    loss_dist = F.mse_loss(pred[:, 0], target[:, 0])\n",
    "    loss_dir  = F.mse_loss(pred[:, 1:], target[:, 1:])\n",
    "    return dist_weight * loss_dist + dir_weight * loss_dir\n",
    "\n",
    "def compute_tre(\n",
    "    meanings : np.ndarray,             # (N,2)   [distance , Œ∏(rad)]\n",
    "    messages : np.ndarray,             # (N,2)   [distance_token , dir_token]\n",
    "    hyperparams: Optional[Dict[str, Any]] = None\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Tree-reconstruction error.  Lower means more compositional.\n",
    "    (We report the best validation loss, like the paper.)\n",
    "    \"\"\"\n",
    "    hp = dict(batch_size=256, val_split=0.2, epochs=300,\n",
    "              lr=1e-2, weight_decay=1e-5, seed=42, emb=32)\n",
    "    if hyperparams: hp.update(hyperparams)\n",
    "    _set_global_seed(hp[\"seed\"])\n",
    "\n",
    "    torch.manual_seed(hp[\"seed\"]);  np.random.seed(hp[\"seed\"])\n",
    "\n",
    "    # ‚îÄ‚îÄ prepare tensors ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    dist_tok = messages[:, 0].astype(np.float32).reshape(-1, 1)\n",
    "    dist_tok = (dist_tok - dist_tok.mean()) / (dist_tok.std() + 1e-8)   # z‚Äëscore\n",
    "    dir_tok  = messages[:, 1].astype(np.int64)\n",
    "    vocab    = int(dir_tok.max()) + 1\n",
    "\n",
    "    # targets: distance  +  angle‚Üí(cos,sin)\n",
    "    y_dist = dist_tok                          # same scale as input (z‚Äëscored)\n",
    "    y_vec  = np.column_stack([np.cos(meanings[:, 1]), np.sin(meanings[:, 1])])\n",
    "    y      = np.column_stack([y_dist, y_vec]).astype(np.float32)\n",
    "\n",
    "    # to torch\n",
    "    Xd = torch.from_numpy(dist_tok)\n",
    "    Xc = torch.from_numpy(dir_tok)\n",
    "    Y  = torch.from_numpy(y)\n",
    "\n",
    "    dataset  = TensorDataset(Xd, Xc, Y)\n",
    "    n_val    = int(hp[\"val_split\"] * len(dataset))\n",
    "    n_train  = len(dataset) - n_val\n",
    "\n",
    "    g = torch.Generator()\n",
    "    g.manual_seed(hp[\"seed\"])\n",
    "    ds_train, ds_val = random_split(dataset, [n_train, n_val], generator=g)\n",
    "\n",
    "    ld_train = DataLoader(ds_train, batch_size=hp[\"batch_size\"], shuffle=True, num_workers=0, generator=torch.Generator().manual_seed(hp[\"seed\"]))\n",
    "    ld_val   = DataLoader(ds_val, batch_size=hp[\"batch_size\"], num_workers=0)\n",
    "\n",
    "    model = _TREModel(vocab, emb=hp[\"emb\"])\n",
    "    opt   = torch.optim.Adam(model.parameters(), lr=hp[\"lr\"],\n",
    "                             weight_decay=hp[\"weight_decay\"])\n",
    "\n",
    "    best = np.inf;  patience = 25;  bad = 0\n",
    "    for _ in range(hp[\"epochs\"]):\n",
    "        model.train()\n",
    "        for xd, xc, y in ld_train:\n",
    "            loss = _loss_tre(model(xd, xc), y, hp[\"dist_weight\"], hp[\"dir_weight\"])\n",
    "            opt.zero_grad(); loss.backward(); opt.step()\n",
    "\n",
    "        # validation\n",
    "        model.eval();  val = 0.0\n",
    "        with torch.no_grad():\n",
    "            for xd, xc, y in ld_val:\n",
    "                val += _loss_tre(model(xd, xc), y, hp[\"dist_weight\"], hp[\"dir_weight\"]).item() * len(y)\n",
    "        val /= n_val\n",
    "\n",
    "        if val < best - 1e-4:\n",
    "            best, bad = val, 0\n",
    "        else:\n",
    "            bad += 1\n",
    "            if bad >= patience: break          # early stop\n",
    "\n",
    "    return float(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9bb4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 8\n",
    "token_directions = bee_interaction_obj.message[:, :8].argmax(dim=-1)\n",
    "token_distances = bee_interaction_obj.message[:, -1]\n",
    "messages = torch.stack([token_directions, token_distances], dim=1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb41ca43",
   "metadata": {},
   "outputs": [],
   "source": [
    "meaning = extract_meaning_spaces(bee_interaction_obj, 'hop_count_distance_vector_sum_direction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb353ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "tre_distance_only = compute_tre(meaning, messages, {'dist_weight': 1.0, 'dir_weight': 0.0})\n",
    "tre_direction_only = compute_tre(meaning, messages, {'dist_weight': 0.0, 'dir_weight': 1.0})\n",
    "tre_distance_heavy = compute_tre(meaning, messages, {'dist_weight': 2.0, 'dir_weight': 1.0})\n",
    "tre_direction_heavy = compute_tre(meaning, messages, {'dist_weight': 1.0, 'dir_weight': 2.0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abb8b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "total, per_tok = compute_posdis(meaning, messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe28579c",
   "metadata": {},
   "source": [
    "### Probing Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded3590f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from scipy.stats import spearmanr\n",
    "import networkx as nx\n",
    "import torch\n",
    "\n",
    "def probe_distance_token(interaction_obj, epsilon=1e-6):\n",
    "    \"\"\"\n",
    "    Returns a dict with Spearman correlations between the continuous distance\n",
    "    token and three candidate graph metrics:\n",
    "        * hop count\n",
    "        * cumulative edge distance\n",
    "        * log(cumulative edge distance)\n",
    "    \"\"\"\n",
    "    distance_token = interaction_obj.message[:, 8].cpu().numpy()\n",
    "    hop_counts     = []\n",
    "    path_costs     = []\n",
    "\n",
    "    global_index = 0\n",
    "    data_batches  = interaction_obj.aux_input['data']\n",
    "    nest_indices  = interaction_obj.aux_input['nest_idx']\n",
    "    food_indices  = interaction_obj.aux_input['food_idx']\n",
    "\n",
    "    for batch in data_batches:\n",
    "        graphs_in_batch = batch.ptr.size(0) - 1\n",
    "\n",
    "        num_nodes = batch.x.size(0)\n",
    "        G = nx.Graph()\n",
    "        G.add_nodes_from(range(num_nodes))\n",
    "        edge_index = batch.edge_index.t().cpu().numpy() \n",
    "        edge_dist  = batch.edge_attr[:, 0].cpu().numpy() \n",
    "        for (u,v), w in zip(edge_index, edge_dist):\n",
    "            G.add_edge(int(u), int(v), weight=float(w))\n",
    "\n",
    "        for g in range(graphs_in_batch):\n",
    "            batch_start = int(batch.ptr[g])\n",
    "            batch_end   = int(batch.ptr[g+1])\n",
    "            sub_nodes = range(batch_start, batch_end)\n",
    "\n",
    "            nest_global = int(nest_indices[global_index + g])\n",
    "            food_global = int(food_indices[global_index + g])\n",
    "\n",
    "            # Shortest-path in unweighted graph -> hop count\n",
    "            hop_count = nx.shortest_path_length(G, source=nest_global,\n",
    "                                                target=food_global)\n",
    "\n",
    "            # Shortest-path with edge weights -> path cost\n",
    "            path      = nx.shortest_path(G, source=nest_global,\n",
    "                                         target=food_global, weight='weight')\n",
    "            cost = 0.0\n",
    "            for u,v in zip(path[:-1], path[1:]):\n",
    "                cost += G[u][v]['weight']\n",
    "\n",
    "            hop_counts.append(hop_count)\n",
    "            path_costs.append(cost)\n",
    "\n",
    "        global_index += graphs_in_batch\n",
    "\n",
    "    hop_counts = np.array(hop_counts)\n",
    "    path_costs = np.array(path_costs)\n",
    "    log_costs  = np.log(path_costs + epsilon)\n",
    "\n",
    "    rho_hop,  _ = spearmanr(hop_counts, distance_token)\n",
    "    rho_cost, _ = spearmanr(path_costs,  distance_token)\n",
    "    rho_log,  _ = spearmanr(log_costs,   distance_token)\n",
    "\n",
    "    return {\n",
    "        \"rho_hop\":  rho_hop,\n",
    "        \"rho_cost\": rho_cost,\n",
    "        \"rho_log_cost\": rho_log\n",
    "    }\n",
    "for label, obj in interactions:\n",
    "    res = probe_distance_token(obj)\n",
    "    print(f\"{label:7s} | œÅ_hop = {res['rho_hop']:.3f}   \"\n",
    "          f\"œÅ_cost = {res['rho_cost']:.3f}   \"\n",
    "          f\"œÅ_log = {res['rho_log_cost']:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e82388",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_graph_metrics(interaction_obj):\n",
    "    \"\"\"Return hop_count, path_cost, log_path_cost, distance_token arrays (N,)\"\"\"\n",
    "    distance_token = interaction_obj.message[:, 8].cpu().numpy()\n",
    "\n",
    "    hop_counts   = []\n",
    "    path_lengths = []\n",
    "\n",
    "    global_counter = 0\n",
    "    data_batches  = interaction_obj.aux_input['data']\n",
    "    nest_idx      = interaction_obj.aux_input['nest_idx']\n",
    "    food_idx      = interaction_obj.aux_input['food_idx']\n",
    "\n",
    "    for batch in data_batches:\n",
    "        graphs_in_batch = batch.ptr.size(0) - 1\n",
    "        num_nodes = batch.x.size(0)\n",
    "        G = nx.Graph()\n",
    "        G.add_nodes_from(range(num_nodes))\n",
    "\n",
    "        edge_index = batch.edge_index.t().cpu().numpy()\n",
    "        edge_dist  = batch.edge_attr[:, 0].cpu().numpy()\n",
    "        for (u,v), w in zip(edge_index, edge_dist):\n",
    "            G.add_edge(int(u), int(v), weight=float(w))\n",
    "\n",
    "        for g in range(graphs_in_batch):\n",
    "            nest_global = int(nest_idx[global_counter + g])\n",
    "            food_global = int(food_idx[global_counter + g])\n",
    "\n",
    "            # Unweighted shortest path -> hop count\n",
    "            hop_counts.append(nx.shortest_path_length(G, \n",
    "                                                      source=nest_global, \n",
    "                                                      target=food_global))\n",
    "\n",
    "            # Weighted shortest path -> cumulative distance\n",
    "            path = nx.shortest_path(G, source=nest_global, \n",
    "                                    target=food_global, weight='weight')\n",
    "            cost = sum(G[u][v]['weight'] for u, v in zip(path[:-1], path[1:]))\n",
    "            path_lengths.append(cost)\n",
    "\n",
    "        global_counter += graphs_in_batch\n",
    "\n",
    "    hop_counts   = np.array(hop_counts)\n",
    "    path_lengths = np.array(path_lengths)\n",
    "    log_lengths  = np.log(path_lengths + 1e-6)\n",
    "\n",
    "    return hop_counts, path_lengths, log_lengths, distance_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbd0523",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_seeds = len(interactions)\n",
    "fig, axes = plt.subplots(nrows=num_seeds, ncols=3, figsize=(15, 4*num_seeds))\n",
    "\n",
    "for row, (label, inter) in enumerate(interactions):\n",
    "    hop, cost, log_cost, token = extract_graph_metrics(inter)\n",
    "\n",
    "    rho_hop, _ = spearmanr(hop, token)\n",
    "    ax = axes[row, 0]\n",
    "    ax.scatter(hop, token, alpha=0.3)\n",
    "    ax.set_xlabel(\"Hop Count\")\n",
    "    ax.set_ylabel(\"Distance Token\")\n",
    "    ax.set_title(f\"{label} (œÅ={rho_hop:.2f})\")\n",
    "\n",
    "    rho_cost, _ = spearmanr(cost, token)\n",
    "    ax = axes[row, 1]\n",
    "    ax.scatter(cost, token, alpha=0.3)\n",
    "    ax.set_xlabel(\"Cumulative Edge Distance\")\n",
    "    ax.set_ylabel(\"Distance Token\")\n",
    "    ax.set_title(f\"{label} (œÅ={rho_cost:.2f})\")\n",
    "\n",
    "    rho_log, _ = spearmanr(log_cost, token)\n",
    "    ax = axes[row, 2]\n",
    "    ax.scatter(log_cost, token, alpha=0.3)\n",
    "    ax.set_xlabel(\"log(Cumulative Edge Distance)\")\n",
    "    ax.set_ylabel(\"Distance Token\")\n",
    "    ax.set_title(f\"{label} (œÅ={rho_log:.2f})\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf53fcce",
   "metadata": {},
   "source": [
    "The continuous slot is not bearing interpretable path‚Äëdistance meaning in our current setup which means agents rely almost entirely on the discrete direction symbol."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655b6b13",
   "metadata": {},
   "source": [
    "# Compositionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d2e928",
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_root    = \"../logs/interactions/2025-07-02\"\n",
    "seed_folders = [\n",
    "    # # baseline\n",
    "    'gamesize10_bee_gs_seed42',\n",
    "    'gamesize10_bee_gs_seed123', \n",
    "    'gamesize10_bee_gs_seed2025',\n",
    "    'gamesize10_bee_gs_seed31', \n",
    "    'gamesize10_bee_gs_seed27',\n",
    "    # binned distance\n",
    "    # \"binneddistance_bee_gs_seed27\",\n",
    "    # \"binneddistance_bee_gs_seed31\",\n",
    "    # \"binneddistance_bee_gs_seed42\",\n",
    "    # \"binneddistance_bee_gs_seed123\",\n",
    "    # \"binneddistance_bee_gs_seed2025\"\n",
    "]\n",
    "hypotheses = [\n",
    "    \"coordinates\",\n",
    "    \"hop_count_distance_vector_sum_direction\",\n",
    "    \"sum_distances_vector_sum_direction\",\n",
    "    \"hop_count_distance_angle_direction\",\n",
    "    \"sum_distances_angle_direction\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34dabae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"results\", exist_ok=True)\n",
    "rows = []\n",
    "\n",
    "for seed_folder in seed_folders:\n",
    "    interaction = load_latest_interaction_file(logs_root, seed_folder)\n",
    "\n",
    "    token_dirs  = interaction.message[:, :8].argmax(dim=-1)\n",
    "    token_dists = interaction.message[:, -1]\n",
    "    emerged_msgs = torch.stack([token_dirs, token_dists], dim=1).numpy()\n",
    "    print(f\"Processing {seed_folder}: {len(emerged_msgs)} messages\")\n",
    "\n",
    "    rng = np.random.RandomState(int(seed_folder.split(\"seed\")[-1]))\n",
    "    seed = int(seed_folder.split(\"seed\")[-1])\n",
    "    print(f\"Seed: {seed}\")\n",
    "\n",
    "    for hyp in hypotheses:\n",
    "        meaning = extract_meaning_spaces(interaction, hyp)\n",
    "\n",
    "        tl = compute_topsim(emerged_msgs, meaning, normalize=\"linear\")\n",
    "        tg = compute_topsim(emerged_msgs, meaning, normalize=\"log\")\n",
    "        tr = compute_topsim(emerged_msgs, meaning, normalize=\"rank\")\n",
    "\n",
    "        ptot, pbd = compute_posdis(emerged_msgs, meaning)\n",
    "        tre       = compute_tre(meaning, emerged_msgs, {'dist_weight': 1.0, 'dir_weight': 1.0})\n",
    "        tre_dist_only    = compute_tre(meaning,    emerged_msgs,\n",
    "                                       {'dist_weight':1.0, 'dir_weight':0.0})\n",
    "        tre_dir_only     = compute_tre(meaning,    emerged_msgs,\n",
    "                                       {'dist_weight':0.0, 'dir_weight':1.0})\n",
    "        tre_dist_heavy   = compute_tre(meaning,    emerged_msgs,\n",
    "                                       {'dist_weight':2.0, 'dir_weight':1.0})\n",
    "        tre_dir_heavy    = compute_tre(meaning,    emerged_msgs,\n",
    "                                       {'dist_weight':1.0, 'dir_weight':2.0})\n",
    "\n",
    "        rows.append({\n",
    "            'seed':               seed_folder,\n",
    "            'hypothesis':         hyp,\n",
    "            'topsim_lin':         tl,\n",
    "            'topsim_log':         tg,\n",
    "            'topsim_rank':        tr,\n",
    "            'posdis_total':       ptot,\n",
    "            'posdis_distance':    pbd['distance_token'],\n",
    "            'posdis_direction':   pbd['direction_token'],\n",
    "            'tre':                tre,\n",
    "            'tre_dist_only':      tre_dist_only,\n",
    "            'tre_dir_only':       tre_dir_only,\n",
    "            'tre_dist_heavy':     tre_dist_heavy,\n",
    "            'tre_dir_heavy':      tre_dir_heavy\n",
    "        })\n",
    "\n",
    "    # random sanity check\n",
    "    N = len(emerged_msgs)\n",
    "    rand_logits = rng.randn(N, 8)\n",
    "    rand_dist   = rng.rand(N, 1) * 10\n",
    "    rand_dir    = rand_logits.argmax(axis=-1).reshape(-1, 1)\n",
    "    rand_msgs   = np.hstack([rand_dist, rand_dir])\n",
    "\n",
    "    rand_true_d = rng.rand(N) * 10\n",
    "    rand_true_a = rng.rand(N) * 2*np.pi - np.pi\n",
    "    rand_mean   = np.vstack([rand_true_d, rand_true_a]).T\n",
    "\n",
    "    r_l   = compute_topsim(rand_msgs, rand_mean, normalize=\"linear\")\n",
    "    r_g   = compute_topsim(rand_msgs, rand_mean, normalize=\"log\")\n",
    "    r_r   = compute_topsim(rand_msgs, rand_mean, normalize=\"rank\")\n",
    "    rp, rpd = compute_posdis(rand_msgs, rand_mean)\n",
    "    r_tre = compute_tre(rand_mean, rand_msgs, {'dist_weight': 1.0, 'dir_weight': 1.0})\n",
    "\n",
    "    rows.append({\n",
    "        'seed':               seed_folder,\n",
    "        'hypothesis':         'random',\n",
    "        'topsim_lin':         r_l,\n",
    "        'topsim_log':         r_g,\n",
    "        'topsim_rank':        r_r,\n",
    "        'posdis_total':       rp,\n",
    "        'posdis_distance':    rpd['distance_token'],\n",
    "        'posdis_direction':   rpd['direction_token'],\n",
    "        'tre':                r_tre\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(rows, columns=[\n",
    "    'seed',\n",
    "    'hypothesis',\n",
    "    'topsim_lin',\n",
    "    'topsim_log',\n",
    "    'topsim_rank',\n",
    "    'posdis_total',\n",
    "    'posdis_distance',\n",
    "    'posdis_direction',\n",
    "    'tre',\n",
    "    'tre_dist_only',\n",
    "    'tre_dir_only',\n",
    "    'tre_dist_heavy',\n",
    "    'tre_dir_heavy'\n",
    "])\n",
    "df.to_csv(\"../results/bee_compositionality.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca7fe6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.to_csv(\"bee_compositionality.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5807c1",
   "metadata": {},
   "source": [
    "# Human Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36dd0b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_interaction_obj = load_latest_interaction_file(\n",
    "    logs_root=\"../logs/interactions/2025-06-22\",\n",
    "    seed_folder=\"maxlen10_human_gs_seed42\",\n",
    "    split=\"validation\",\n",
    "    prefix=\"interaction_gpu0\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea41771",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_interaction_obj.message.argmax(-1)[:, :-1].cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7479a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD = 0\n",
    "DIRECTIONS = { \"N\":0,  \"NE\":1,  \"E\":2,  \"SE\":3,\n",
    "               \"S\":4,  \"SW\":5,  \"W\":6,  \"NW\":7 }\n",
    "\n",
    "def intify(seq, vocab):\n",
    "    \"\"\"Map symbols to ints ‚â•1; PAD stays 0.\"\"\"\n",
    "    out = []\n",
    "    for s in seq:\n",
    "        if s not in vocab:\n",
    "            vocab[s] = len(vocab) + 1\n",
    "        out.append(vocab[s])\n",
    "    return out\n",
    "\n",
    "def pad_to_max_len(seqs, max_len):\n",
    "    \"\"\"Right-pad or truncate each int sequence to max_len.\"\"\"\n",
    "    M = len(seqs)\n",
    "    out = np.zeros((M, max_len), dtype=int)\n",
    "    for i, s in enumerate(seqs):\n",
    "        ln = min(len(s), max_len)\n",
    "        out[i,:ln] = s[:ln]\n",
    "    return out\n",
    "\n",
    "def extract_truth_sequences(interaction, max_len, dist_bins=3):\n",
    "    \"\"\"\n",
    "    For each example in interaction:\n",
    "      - Enumerate all fewest-hop paths (nx.all_shortest_paths)\n",
    "      - Tie-break by summing each path's G[u][v]['distance'] ‚Üí pick minimal\n",
    "      - Read G[u][v]['direction'] ‚Üí 0..7 via DIRECTIONS\n",
    "      - Collect raw distances, bin globally into `dist_bins` percentile bins ‚Üí 0..dist_bins-1\n",
    "      - Pad/truncate both sequences to max_len (PAD=0)\n",
    "    Returns:\n",
    "      truth_dirs, truth_dists: List[List[int]] each of length max_len\n",
    "    \"\"\"\n",
    "    # 1) gather graphs & indices\n",
    "    data_list, raw_paths = [], []\n",
    "    for batch in interaction.aux_input['data']:\n",
    "        data_list.extend(batch.to_data_list())\n",
    "    nests = interaction.aux_input['nest_idx'].tolist()\n",
    "    foods = interaction.aux_input['food_idx'].tolist()\n",
    "    if len(data_list) != len(nests):\n",
    "        raise ValueError(\"mismatch data vs indices\")\n",
    "\n",
    "    # 2) collect all raw distances for binning later\n",
    "    all_dists = []\n",
    "    for i, data in enumerate(data_list):\n",
    "        G = build_graph(data)\n",
    "        s, t = nests[i], foods[i]\n",
    "        # all fewest‚Äêhop paths\n",
    "        cands = list(nx.all_shortest_paths(G, source=s, target=t))\n",
    "        # pick by minimal sum of edge‚Äêdistance\n",
    "        def cost(path):\n",
    "            return sum(G[u][v]['distance'] for u,v in zip(path,path[1:]))\n",
    "        best = min(cands, key=cost)\n",
    "        # record its edges\n",
    "        eds = []\n",
    "        for u,v in zip(best, best[1:]):\n",
    "            raw = G[u][v]['distance']\n",
    "            all_dists.append(raw)\n",
    "            eds.append((G[u][v]['direction'], raw))\n",
    "        raw_paths.append(eds)\n",
    "\n",
    "    # 3) global percentile bins for distances\n",
    "    pct = np.linspace(0,100,dist_bins+1)\n",
    "    edges = np.percentile(all_dists, pct)\n",
    "    edges[-1] += 1e-12  # close last bin\n",
    "\n",
    "    # 4) build discrete sequences\n",
    "    truth_dirs, truth_dists = [], []\n",
    "    for eds in raw_paths:\n",
    "        dirs, dists = [], []\n",
    "        for dstr, raw in eds:\n",
    "            dirs.append(DIRECTIONS[dstr])\n",
    "            cat = int(np.digitize(raw, edges)) - 1\n",
    "            dists.append(cat)\n",
    "        # pad/truncate\n",
    "        if len(dirs) < max_len:\n",
    "            padlen = max_len - len(dirs)\n",
    "            dirs  += [PAD]*padlen\n",
    "            dists += [PAD]*padlen\n",
    "        else:\n",
    "            dirs  = dirs[:max_len]\n",
    "            dists = dists[:max_len]\n",
    "        truth_dirs.append(dirs)\n",
    "        truth_dists.append(dists)\n",
    "\n",
    "    return truth_dirs, truth_dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f39e029",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_posdis_seq(msg_seqs, truth_dirs, truth_dists):\n",
    "    \"\"\"\n",
    "    For each slot p:\n",
    "      MI_dir = I(Msg_p;Dir_p)\n",
    "      MI_dst = I(Msg_p;Dist_p)\n",
    "      posdis_p = (max(MI_dir,MI_dst) - min(...)) / H(Msg_p)\n",
    "    Return (mean_posdis, per_slot_list).\n",
    "    \"\"\"\n",
    "    X = np.array(msg_seqs,     dtype=int)\n",
    "    D = np.array(truth_dirs,   dtype=int)\n",
    "    Z = np.array(truth_dists,  dtype=int)\n",
    "    N, L = X.shape\n",
    "\n",
    "    # entropy per slot\n",
    "    H = np.zeros(L)\n",
    "    for p in range(L):\n",
    "        _, cnt = np.unique(X[:,p], return_counts=True)\n",
    "        probs = cnt / cnt.sum()\n",
    "        H[p] = entropy(probs)\n",
    "\n",
    "    # compute MI per slot\n",
    "    scores = []\n",
    "    for p in range(L):\n",
    "        tok = X[:, p]\n",
    "        mi_dir  = mutual_info_score(tok, D[:, p])\n",
    "        mi_dist = mutual_info_score(tok, Z[:, p])\n",
    "        top1, top2 = max(mi_dir,mi_dist), min(mi_dir,mi_dist)\n",
    "        scores.append((top1 - top2)/H[p] if H[p]>0 else 0.0)\n",
    "\n",
    "    return float(np.mean(scores)), scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8538df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TRESeq(nn.Module):\n",
    "    \"\"\"\n",
    "    Takes a derivation sequence of integers (concatenated dir+dist tokens),\n",
    "    embeds and sums them, then predicts the full message sequence via a single linear head.\n",
    "    \"\"\"\n",
    "    def __init__(self, deriv_vocab_size: int, msg_vocab_size: int, seq_len: int, emb: int = 64):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(deriv_vocab_size, emb, padding_idx=PAD)\n",
    "        self.head = nn.Linear(emb, seq_len * msg_vocab_size, bias=False)\n",
    "        self.seq_len = seq_len\n",
    "        self.msg_vocab_size = msg_vocab_size\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        z = self.emb(x).sum(dim=1)\n",
    "        out = self.head(z)\n",
    "        return out.view(-1, self.seq_len, self.msg_vocab_size)\n",
    "\n",
    "def compute_tre_seq(\n",
    "    deriv_seqs: list[list[int]],\n",
    "    msg_seqs:   list[list[int]],\n",
    "    tre_hp:     dict | None = None\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Train TRESeq to predict `msg_seqs` from `deriv_seqs`.\n",
    "    Returns the best validation cross‚Äêentropy loss (lower ‚áí more compositional).\n",
    "    \n",
    "    deriv_seqs : List of integer lists (the concatenated dir+dist derivation tokens)\n",
    "    msg_seqs   : List of integer lists (the actual message token sequences)\n",
    "    tre_hp     : Optional hyperparam overrides, e.g. {'seed':123, 'epochs':100}\n",
    "    \"\"\"\n",
    "    # 1) default hyperparameters\n",
    "    hp = {\n",
    "        'seed': 42,\n",
    "        'emb': 64,\n",
    "        'batch_size': 256,\n",
    "        'val_split': 0.2,\n",
    "        'epochs': 200,\n",
    "        'lr': 1e-2,\n",
    "        'weight_decay': 1e-5,\n",
    "        'patience': 20,\n",
    "    }\n",
    "    if tre_hp:\n",
    "        hp.update(tre_hp)\n",
    "    _set_global_seed(hp['seed'])\n",
    "\n",
    "    # 2) build vocab sizes\n",
    "    deriv_vocab_size = max(max(seq) for seq in deriv_seqs) + 1\n",
    "    msg_vocab_size   = max(max(seq) for seq in msg_seqs)   + 1\n",
    "    L_deriv = max(len(seq) for seq in deriv_seqs)\n",
    "    L_msg   = max(len(seq) for seq in msg_seqs)\n",
    "\n",
    "    # 3) pad sequences to fixed length\n",
    "    def pad(seqs, L):\n",
    "        arr = np.full((len(seqs), L), PAD, dtype=np.int64)\n",
    "        for i, s in enumerate(seqs):\n",
    "            ln = min(len(s), L)\n",
    "            arr[i, :ln] = s[:ln]\n",
    "        return arr\n",
    "    X_deriv = pad(deriv_seqs, L_deriv)\n",
    "    Y_msg   = pad(msg_seqs,   L_msg)\n",
    "\n",
    "    # 4) create PyTorch dataset & split\n",
    "    X_t = torch.from_numpy(X_deriv)\n",
    "    Y_t = torch.from_numpy(Y_msg)\n",
    "    ds = TensorDataset(X_t, Y_t)\n",
    "    n_val = int(hp['val_split'] * len(ds))\n",
    "    n_tr  = len(ds) - n_val\n",
    "    g = torch.Generator().manual_seed(hp['seed'])\n",
    "    tr_ds, vl_ds = random_split(ds, [n_tr, n_val], generator=g)\n",
    "    tr_ld = DataLoader(tr_ds, batch_size=hp['batch_size'], shuffle=True,\n",
    "                       generator=torch.Generator().manual_seed(hp['seed']))\n",
    "    vl_ld = DataLoader(vl_ds, batch_size=hp['batch_size'])\n",
    "\n",
    "    # 5) model & optimizer\n",
    "    model = TRESeq(deriv_vocab_size, msg_vocab_size, L_msg, emb=hp['emb'])\n",
    "    opt   = torch.optim.Adam(model.parameters(),\n",
    "                             lr=hp['lr'],\n",
    "                             weight_decay=hp['weight_decay'])\n",
    "\n",
    "    # 6) training with early stopping\n",
    "    best_loss, bad = float('inf'), 0\n",
    "    for epoch in range(hp['epochs']):\n",
    "        model.train()\n",
    "        for xb, yb in tr_ld:\n",
    "            logits = model(xb)\n",
    "            loss = F.cross_entropy(\n",
    "                logits.view(-1, msg_vocab_size),\n",
    "                yb.view(-1),\n",
    "                ignore_index=PAD\n",
    "            )\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        total    = 0\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in vl_ld:\n",
    "                logits = model(xb)\n",
    "                l = F.cross_entropy(\n",
    "                    logits.view(-1, msg_vocab_size),\n",
    "                    yb.view(-1),\n",
    "                    ignore_index=PAD\n",
    "                )\n",
    "                val_loss += l.item() * yb.numel()\n",
    "                total    += yb.numel()\n",
    "        val_loss /= total\n",
    "\n",
    "        if val_loss < best_loss - 1e-4:\n",
    "            best_loss, bad = val_loss, 0\n",
    "        else:\n",
    "            bad += 1\n",
    "            if bad >= hp['patience']:\n",
    "                break\n",
    "\n",
    "    return best_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63500fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _ed(a, b):\n",
    "    \"\"\"Levenshtein edit distance on integer tuples.\"\"\"\n",
    "    m,n = len(a), len(b)\n",
    "    dp = list(range(n+1))\n",
    "    for i in range(1,m+1):\n",
    "        prev, dp[0] = dp[0], i\n",
    "        for j in range(1,n+1):\n",
    "            cost = 0 if a[i-1]==b[j-1] else 1\n",
    "            cur  = min(prev+cost, dp[j]+1, dp[j-1]+1)\n",
    "            prev, dp[j] = dp[j], cur\n",
    "    return dp[-1]\n",
    "\n",
    "def compute_topsim_seq(msg_seqs, truth_seqs):\n",
    "    \"\"\"\n",
    "    Spearman‚ÄêœÅ between:\n",
    "      msg_dist(i,j)   = edit_dist(msg_i,msg_j)/L\n",
    "      truth_dist(i,j) = edit_dist(truth_i,truth_j)/L\n",
    "    Now with nan_policy=\"raise\".\n",
    "    \"\"\"\n",
    "    X = np.array(msg_seqs,   dtype=int)\n",
    "    Y = np.array(truth_seqs, dtype=int)\n",
    "    N, L = X.shape\n",
    "\n",
    "    msg_d = pdist(X, metric=lambda a,b: _ed(tuple(a),tuple(b))/L)\n",
    "    truth_d = pdist(Y, metric=lambda a,b: _ed(tuple(a),tuple(b))/L)\n",
    "\n",
    "    return spearmanr(msg_d, truth_d, nan_policy=\"raise\").correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2937a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "human_seeds = [\n",
    "    \"logs/interactions/2025-07-08/maxlen2_vocab100_human_gs_seed27\",\n",
    "    \"logs/interactions/2025-07-08/maxlen2_vocab100_human_gs_seed31\",\n",
    "    \"logs/interactions/2025-06-22/maxlen2_human_gs_seed42\",\n",
    "    \"logs/interactions/2025-06-23/human_maxlen_sweep_max_len2_seed123\",\n",
    "    \"logs/interactions/2025-06-23/human_maxlen_sweep_max_len2_seed2025\",\n",
    "\n",
    "    \"logs/interactions/2025-07-08/maxlen4_vocab100_human_gs_seed27\",\n",
    "    \"logs/interactions/2025-07-08/maxlen4_vocab100_human_gs_seed31\",\n",
    "    \"logs/interactions/2025-06-22/maxlen4_human_gs_seed42\",\n",
    "    \"logs/interactions/2025-06-23/human_maxlen_sweep_max_len4_seed123\",\n",
    "    \"logs/interactions/2025-06-23/human_maxlen_sweep_max_len4_seed2025\",\n",
    "\n",
    "    \"logs/interactions/2025-07-08/maxlen6_vocab100_human_gs_seed27\",\n",
    "    \"logs/interactions/2025-07-08/maxlen6_vocab100_human_gs_seed31\",\n",
    "    \"logs/interactions/2025-06-22/maxlen6_human_gs_seed42\",\n",
    "    \"logs/interactions/2025-06-23/human_maxlen_sweep_max_len6_seed123\",\n",
    "    \"logs/interactions/2025-06-23/human_maxlen_sweep_max_len6_seed2025\",\n",
    "\n",
    "    \"logs/interactions/2025-07-08/maxlen10_vocab100_human_gs_seed27\",\n",
    "    \"logs/interactions/2025-07-08/maxlen10_vocab100_human_gs_seed31\",\n",
    "    \"logs/interactions/2025-06-22/maxlen10_human_gs_seed42\",\n",
    "    \"logs/interactions/2025-06-23/human_maxlen_sweep_max_len10_seed123\",\n",
    "    \"logs/interactions/2025-06-23/human_maxlen_sweep_max_len10_seed2025\"\n",
    "]\n",
    "\n",
    "logs_root = \"..\"\n",
    "_set_global_seed(42)\n",
    "rows = []\n",
    "for seed_folder in human_seeds:\n",
    "    m = re.search(r\"max[_]?len[_]?(\\d+)\", seed_folder)\n",
    "    if not m:\n",
    "        raise ValueError(f\"Could not parse max_len from '{seed_folder}'\")\n",
    "    max_len = int(m.group(1))\n",
    "\n",
    "    interaction = load_latest_interaction_file(\n",
    "        logs_root=logs_root,\n",
    "        seed_folder=seed_folder,\n",
    "        split='validation',\n",
    "        prefix='interaction_gpu0'\n",
    "    )\n",
    "\n",
    "    truth_dirs, truth_dists = extract_truth_sequences(\n",
    "        interaction,\n",
    "        max_len=max_len,\n",
    "        dist_bins=3\n",
    "    )\n",
    "\n",
    "    msg_ids   = interaction.message.argmax(-1)[:, :-1].cpu().numpy()\n",
    "    msg_seqs  = [ list(row[:max_len]) for row in msg_ids ]\n",
    "\n",
    "    topsim_actual   = compute_topsim_seq(msg_seqs, truth_dirs)\n",
    "    posdis_actual,_ = compute_posdis_seq(msg_seqs, truth_dirs, truth_dists)\n",
    "\n",
    "    truth_dirs_shift  = [[d+1 for d in seq] for seq in truth_dirs]\n",
    "    truth_dists_shift = [[z+1 for z in seq] for seq in truth_dists]\n",
    "\n",
    "    offset = len(DIRECTIONS) \n",
    "    deriv_seqs = []\n",
    "    for dseq, zseq in zip(truth_dirs_shift, truth_dists_shift):\n",
    "        seq = []\n",
    "        for d, z in zip(dseq, zseq):\n",
    "            if d != PAD: \n",
    "                seq.append(d)\n",
    "            if z != PAD:\n",
    "                seq.append(z + offset)\n",
    "        deriv_seqs.append(seq)\n",
    "\n",
    "    tre_actual = compute_tre_seq(deriv_seqs, msg_seqs, tre_hp={'seed':42})\n",
    "\n",
    "    rows.append({\n",
    "        'seed_folder': seed_folder,\n",
    "        'max_len':     max_len,\n",
    "        'type':        'actual',\n",
    "        'TopSim':      topsim_actual,\n",
    "        'PosDis':      posdis_actual,\n",
    "        'TRE':         tre_actual\n",
    "    })\n",
    "\n",
    "    unique_tokens = sorted({t for seq in msg_seqs for t in seq})\n",
    "    np.random.seed(42)\n",
    "    rand_seqs = [\n",
    "        list(np.random.choice(unique_tokens, size=max_len))\n",
    "        for _ in msg_seqs\n",
    "    ]\n",
    "\n",
    "    topsim_rand   = compute_topsim_seq(rand_seqs, truth_dirs)\n",
    "    posdis_rand,_ = compute_posdis_seq(rand_seqs, truth_dirs, truth_dists)\n",
    "    tre_rand      = compute_tre_seq(deriv_seqs, rand_seqs, tre_hp={'seed':42})\n",
    "\n",
    "    rows.append({\n",
    "        'seed_folder': seed_folder,\n",
    "        'max_len':     max_len,\n",
    "        'type':        'random',\n",
    "        'TopSim':      topsim_rand,\n",
    "        'PosDis':      posdis_rand,\n",
    "        'TRE':         tre_rand\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "df.to_csv(\"human_compositionality.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44d887a",
   "metadata": {},
   "source": [
    "## Zero-Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38b754c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({\n",
    "    \"font.size\": 16,\n",
    "    \"axes.titlesize\": 16,\n",
    "    \"axes.labelsize\": 16,\n",
    "    \"xtick.labelsize\": 16,\n",
    "    \"ytick.labelsize\": 16,\n",
    "    \"legend.fontsize\": 16,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f989777",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_validation_accuracy(run_name, root=\"../logs/csv/\"):\n",
    "    \"function to get the best validation accuracy from specified files\"\n",
    "    csv_path = os.path.join(root, f\"{run_name}.csv\")\n",
    "    if not os.path.isfile(csv_path):\n",
    "        print(f\"[WARN] CSV not found: {csv_path}\")\n",
    "        return None\n",
    "\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    if {\"mode\", \"acc\"}.issubset(df.columns):\n",
    "        best = df.loc[df[\"mode\"] == \"test\", \"acc\"].max()\n",
    "        return float(best)\n",
    "    else:\n",
    "        raise KeyError(f\"Columns 'mode' and 'acc' not found in {csv_path}\")\n",
    "\n",
    "runs = [\n",
    "    \"v2_tzsg_5nodes_bee_seed27\",\n",
    "    \"v2_tzsg_5nodes_bee_seed31\",\n",
    "    \"v2_tzsg_5nodes_bee_seed42\",\n",
    "    \"v2_tzsg_5nodes_bee_seed2025\",\n",
    "    \"v2_tzsg_5nodes_bee_seed123\",\n",
    "    \"v2_tzsg_5nodes_human_seed27\",\n",
    "    \"v2_tzsg_5nodes_human_seed42\",\n",
    "    \"v2_tzsg_5nodes_human_seed123\",\n",
    "    \"v2_tzsg_5nodes_human_seed2025\",\n",
    "    \"v2_tzsg_5nodes_human_seed31\"\n",
    "]\n",
    "\n",
    "for run in runs:\n",
    "    acc = best_validation_accuracy(run)\n",
    "    if acc is not None:\n",
    "        print(f\"{run:30s}  best-val-acc = {acc*100:.2f} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2239e245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting 1-hop zero-shot figure\n",
    "\n",
    "bee_val_5 = np.array([94.40, 95.40, 95.70, 94.80, 95.60])\n",
    "human_val_5 = np.array([96.50, 95.50, 95.80, 97.00, 96.30])\n",
    "bee_val_10 = np.array([93.70, 93.10, 93.00, 92.90, 93.10])\n",
    "human_val_10 = np.array([90.70, 91.40, 94.00, 94.20, 90.90])\n",
    "\n",
    "bee_test_5 = np.array([37.94, 40.21, 37.43, 36.89, 40.50])\n",
    "human_test_5 = np.array([27.10, 34.30, 38.99, 30.64, 46.00])\n",
    "bee_test_10 = np.array([8.03, 12.04, 7.81, 8.18, 10.33])\n",
    "human_test_10 = np.array([8.25, 8.81, 6.01, 6.52, 11.52])\n",
    "\n",
    "def mean_ci95(arr):\n",
    "    mean = arr.mean()\n",
    "    sem = arr.std(ddof=1) / np.sqrt(len(arr))\n",
    "    t_val = stats.t.ppf(1 - 0.025, len(arr) - 1)\n",
    "    ci95 = sem * t_val\n",
    "    return mean, ci95\n",
    "\n",
    "bee5_val_mean, bee5_val_ci95 = mean_ci95(bee_val_5)\n",
    "human5_val_mean, human5_val_ci95 = mean_ci95(human_val_5)\n",
    "bee10_val_mean, bee10_val_ci95 = mean_ci95(bee_val_10)\n",
    "human10_val_mean, human10_val_ci95 = mean_ci95(human_val_10)\n",
    "\n",
    "bee5_mean, bee5_ci95 = mean_ci95(bee_test_5)\n",
    "human5_mean, human5_ci95 = mean_ci95(human_test_5)\n",
    "bee10_mean, bee10_ci95 = mean_ci95(bee_test_10)\n",
    "human10_mean, human10_ci95 = mean_ci95(human_test_10)\n",
    "\n",
    "groups = [\"5-node graph\", \"10-node graph\"]\n",
    "x = np.arange(len(groups))\n",
    "width = 0.2\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9, 6))\n",
    "\n",
    "ax.bar(x[0] - 1.5 * width, bee5_val_mean, width, yerr=bee5_val_ci95,\n",
    "       capsize=6, label=\"Best Validation Bee\", color=\"#0073e6\",\n",
    "       hatch=\"//\", linewidth=1)\n",
    "ax.bar(x[0] - 0.5 * width, human5_val_mean, width, yerr=human5_val_ci95,\n",
    "       capsize=6, label=\"Best Validation Human\", color=\"#66b2ff\",\n",
    "       hatch=\"//\", linewidth=1)\n",
    "ax.bar(x[0] + 0.5 * width, bee5_mean, width, yerr=bee5_ci95,\n",
    "       capsize=6, label=\"Bee\", color=\"#0073e6\")\n",
    "ax.bar(x[0] + 1.5 * width, human5_mean, width, yerr=human5_ci95,\n",
    "       capsize=6, label=\"Human\", color=\"#66b2ff\")\n",
    "\n",
    "ax.bar(x[1] - 1.5 * width, bee10_val_mean, width, yerr=bee10_val_ci95,\n",
    "       capsize=6, color=\"#0073e6\", hatch=\"//\", linewidth=1)\n",
    "ax.bar(x[1] - 0.5 * width, human10_val_mean, width, yerr=human10_val_ci95,\n",
    "       capsize=6, color=\"#66b2ff\", hatch=\"//\", linewidth=1)\n",
    "ax.bar(x[1] + 0.5 * width, bee10_mean, width, yerr=bee10_ci95,\n",
    "       capsize=6, color=\"#0073e6\")\n",
    "ax.bar(x[1] + 1.5 * width, human10_mean, width, yerr=human10_ci95,\n",
    "       capsize=6, color=\"#66b2ff\")\n",
    "\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(groups)\n",
    "ax.set_ylabel(\"Accuracy (%)\")\n",
    "ax.set_ylim(0, 105)\n",
    "\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "seen, ordered = {}, []\n",
    "for h, l in zip(handles, labels):\n",
    "    if l not in seen:\n",
    "        seen[l] = True\n",
    "        ordered.append((h, l))\n",
    "order_labels = [\"Best Validation Bee\", \"Best Validation Human\", \"Bee\", \"Human\"]\n",
    "ordered_handles = [h for label in order_labels for h, l in ordered if l == label]\n",
    "ax.legend(ordered_handles, order_labels, loc=\"upper center\",\n",
    "          bbox_to_anchor=(0.5, -0.15), ncol=4, frameon=False)\n",
    "\n",
    "bars_per_group = 4 \n",
    "for i, rect in enumerate(ax.patches):\n",
    "    height = rect.get_height()\n",
    "    if height <= 0:\n",
    "        continue\n",
    "    x_pos = rect.get_x() + rect.get_width() / 2\n",
    "\n",
    "    group_index = i % bars_per_group\n",
    "    if group_index == 0:\n",
    "        offset_x = -5\n",
    "    elif group_index == 1:\n",
    "        offset_x = -2\n",
    "    elif group_index == 2:\n",
    "        offset_x = 2\n",
    "    else:\n",
    "        offset_x = 5\n",
    "\n",
    "    ax.annotate(f\"{height:.1f}%\", xy=(x_pos, height),\n",
    "                xytext=(offset_x, 8), textcoords=\"offset points\",\n",
    "                ha=\"center\", va=\"bottom\", fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688e05de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting 2-hop zero-shot\n",
    "\n",
    "bee_val = np.array([95.00, 94.50, 94.60, 94.10, 95.00])\n",
    "human_val = np.array([94.40, 96.20, 96.10, 94.80, 94.10])\n",
    "\n",
    "bee_test_3 = np.array([89.53, 92.50, 92.80, 91.50, 90.82])\n",
    "human_test_3 = np.array([93.36, 93.43, 89.89, 88.60, 91.28])\n",
    "\n",
    "bee_test_4 = np.array([91.11, 91.70, 91.43, 90.09, 89.36])\n",
    "human_test_4 = np.array([94.09, 92.36, 89.82, 86.57, 93.12])\n",
    "\n",
    "bee_val_mean, bee_val_sem = mean_ci95(bee_val)\n",
    "human_val_mean, human_val_sem = mean_ci95(human_val)\n",
    "bee3_mean, bee3_sem = mean_ci95(bee_test_3)\n",
    "human3_mean, human3_sem = mean_ci95(human_test_3)\n",
    "bee4_mean, bee4_sem = mean_ci95(bee_test_4)\n",
    "human4_mean, human4_sem = mean_ci95(human_test_4)\n",
    "\n",
    "groups = [\"Trained on 2-hop\", \"Tested on 3-hop\", \"Tested on 4-hop\"]\n",
    "x = np.arange(len(groups))\n",
    "width = 0.25\n",
    "offset = 0.03 \n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9, 6))\n",
    "\n",
    "bar_bee_val = ax.bar(x[0] - width/2 - offset, bee_val_mean, width,\n",
    "                     yerr=bee_val_sem, capsize=6,\n",
    "                     label=\"Best Validation Bee\", color=\"#0073e6\",\n",
    "                     hatch=\"//\", linewidth=1)\n",
    "bar_human_val = ax.bar(x[0] + width/2 + offset, human_val_mean, width,\n",
    "                       yerr=human_val_sem, capsize=6,\n",
    "                       label=\"Best Validation Human\", color=\"#66b2ff\",\n",
    "                       hatch=\"//\", linewidth=1)\n",
    "bar_bee3 = ax.bar(x[1] - width/2 - offset, bee3_mean, width,\n",
    "                  yerr=bee3_sem, capsize=6,\n",
    "                  label=\"Bee\", color=\"#0073e6\")\n",
    "bar_human3 = ax.bar(x[1] + width/2 + offset, human3_mean, width,\n",
    "                    yerr=human3_sem, capsize=6,\n",
    "                    label=\"Human\", color=\"#66b2ff\")\n",
    "bar_bee4 = ax.bar(x[2] - width/2 - offset, bee4_mean, width,\n",
    "                  yerr=bee4_sem, capsize=6,\n",
    "                  color=\"#0073e6\")\n",
    "bar_human4 = ax.bar(x[2] + width/2 + offset, human4_mean, width,\n",
    "                    yerr=human4_sem, capsize=6,\n",
    "                    color=\"#66b2ff\")\n",
    "\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(groups)\n",
    "ax.set_ylabel(\"Accuracy (%)\")\n",
    "ax.set_ylim(0, 105)\n",
    "\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "seen = {}\n",
    "ordered = []\n",
    "for h, l in zip(handles, labels):\n",
    "    if l not in seen:\n",
    "        seen[l] = True\n",
    "        ordered.append((h, l))\n",
    "order_labels = [\"Best Validation Bee\", \"Best Validation Human\", \"Bee\", \"Human\"]\n",
    "ordered_handles = [h for label in order_labels for h, l in ordered if l == label]\n",
    "ax.legend(ordered_handles, order_labels,\n",
    "          loc=\"upper center\", bbox_to_anchor=(0.5, -0.15),\n",
    "          ncol=4, frameon=False)\n",
    "\n",
    "for rect in ax.patches:\n",
    "    height = rect.get_height()\n",
    "    if height <= 0:\n",
    "        continue\n",
    "    x_pos = rect.get_x() + rect.get_width() / 2\n",
    "    ax.annotate(f\"{height:.1f}%\", xy=(x_pos, height),\n",
    "                xytext=(2, 6), textcoords=\"offset points\",\n",
    "                ha=\"center\", va=\"bottom\", fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b827fa2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"font.size\": 16,\n",
    "    \"axes.titlesize\": 16,\n",
    "    \"axes.labelsize\": 16,\n",
    "    \"xtick.labelsize\": 16,\n",
    "    \"ytick.labelsize\": 16,\n",
    "    \"legend.fontsize\": 16,\n",
    "})\n",
    "\n",
    "def aggregate_length_by_epoch(file_patterns):\n",
    "    seed_series = []\n",
    "    for pattern in file_patterns:\n",
    "        for path in sorted(glob.glob(pattern)):\n",
    "            df = pd.read_csv(path)\n",
    "            df = df[df[\"mode\"] == \"test\"]\n",
    "            if \"length\" not in df.columns or df.empty:\n",
    "                continue\n",
    "            series = df.groupby(\"epoch\")[\"length\"].mean()\n",
    "            seed_label = os.path.basename(path)\n",
    "            series.name = seed_label\n",
    "            seed_series.append(series)\n",
    "    if not seed_series:\n",
    "        raise RuntimeError(f\"No matching test files for patterns {file_patterns}\")\n",
    "    combined = pd.concat(seed_series, axis=1)\n",
    "    mean_length = combined.mean(axis=1, skipna=True)\n",
    "    sem_length = combined.apply(lambda row: row.dropna().std(ddof=1) / np.sqrt(row.dropna().shape[0]), axis=1)\n",
    "    return mean_length.index.to_numpy(), mean_length.to_numpy(), sem_length.to_numpy()\n",
    "\n",
    "cond1_patterns = [\n",
    "    \"../logs/csv/2025-07-31/tzsg_5nodes_human_seed*.csv\",\n",
    "]\n",
    "cond2_patterns = [\n",
    "    \"../logs/csv/2025-07-31/v2_tzsg_5nodes_human_seed*.csv\"\n",
    "]\n",
    "cond3_patterns = [\n",
    "    \"../logs/csv/2025-07-07/gamesize5_maxlen10_human_gs_seed*.csv\",\n",
    "]\n",
    "\n",
    "epochs1, mean1, sem1 = aggregate_length_by_epoch(cond1_patterns)\n",
    "epochs2, mean2, sem2 = aggregate_length_by_epoch(cond2_patterns)\n",
    "epochs3, mean3, sem3 = aggregate_length_by_epoch(cond3_patterns)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9, 6))\n",
    "ax.plot(epochs1, mean1, label=\"1-hop training\", color=\"#0073e6\")\n",
    "ax.fill_between(epochs1, mean1 - sem1, mean1 + sem1, alpha=0.3, color=\"#99ccff\")\n",
    "ax.plot(epochs2, mean2, label=\"2-hop training\", color=\"#ff8000\")\n",
    "ax.fill_between(epochs2, mean2 - sem2, mean2 + sem2, alpha=0.3, color=\"#ffb366\")\n",
    "ax.plot(epochs3, mean3, label=\"Multi-hop training\", color=\"#9966ff\")\n",
    "ax.fill_between(epochs3, mean3 - sem3, mean3 + sem3, alpha=0.3, color=\"#dabfff\")\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"Message Length\")\n",
    "ax.legend(loc=\"upper center\", bbox_to_anchor=(0.5, -0.15), ncol=3, frameon=False)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab90c98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import sem as sem_func\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"font.size\": 16,\n",
    "    \"axes.titlesize\": 16,\n",
    "    \"axes.labelsize\": 16,\n",
    "    \"xtick.labelsize\": 16,\n",
    "    \"ytick.labelsize\": 16,\n",
    "    \"legend.fontsize\": 16,\n",
    "})\n",
    "\n",
    "def compute_empirical_cdf(file_patterns):\n",
    "    seed_cdfs = []\n",
    "    for pattern in file_patterns:\n",
    "        for path in sorted(glob.glob(pattern)):\n",
    "            df = pd.read_csv(path)\n",
    "            df = df[df[\"mode\"] == \"test\"]\n",
    "            if \"length\" not in df.columns or df[\"length\"].dropna().empty:\n",
    "                continue\n",
    "            lengths = df[\"length\"].dropna().to_numpy()\n",
    "            sorted_len = np.sort(lengths)\n",
    "            seed_cdfs.append(sorted_len)\n",
    "    if not seed_cdfs:\n",
    "        raise RuntimeError(f\"No matching files for patterns {file_patterns}\")\n",
    "    all_lengths = np.concatenate(seed_cdfs)\n",
    "    min_l, max_l = all_lengths.min(), all_lengths.max()\n",
    "    grid = np.linspace(min_l, max_l, 200)\n",
    "    cdf_matrix = []\n",
    "    for sorted_len in seed_cdfs:\n",
    "        counts = np.searchsorted(sorted_len, grid, side=\"right\")\n",
    "        cdf = counts / sorted_len.size\n",
    "        cdf_matrix.append(cdf)\n",
    "    cdf_matrix = np.vstack(cdf_matrix)\n",
    "    mean_cdf = np.nanmean(cdf_matrix, axis=0)\n",
    "    sem_cdf = sem_func(cdf_matrix, axis=0, nan_policy=\"omit\")\n",
    "    return grid, mean_cdf, sem_cdf\n",
    "\n",
    "cond1 = [\n",
    "    \"../logs/csv/2025-07-31/tzsg_5nodes_human_seed*.csv\",\n",
    "    \"../logs/csv/2025-07-31/tzsg_5nodes_bee_seed*.csv\",\n",
    "]\n",
    "cond2 = [\n",
    "    \"../logs/csv/2025-07-31/v2_tzsg_5nodes_human_seed*.csv\",\n",
    "    \"../logs/csv/2025-07-31/v2_tzsg_5nodes_bee_seed*.csv\",\n",
    "]\n",
    "cond3 = [\n",
    "    \"../logs/csv/2025-07-07/gamesize5_maxlen10_human_gs_seed*.csv\",\n",
    "    \"../logs/csv/2025-07-07/gamesize5_maxlen10_bee_gs_seed*.csv\",\n",
    "]\n",
    "\n",
    "grid1, mean1, sem1 = compute_empirical_cdf(cond1)\n",
    "grid2, mean2, sem2 = compute_empirical_cdf(cond2)\n",
    "grid3, mean3, sem3 = compute_empirical_cdf(cond3)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9, 6))\n",
    "ax.step(grid1, mean1, where=\"post\", label=\"1-hop training\", color=\"#0073e6\")\n",
    "ax.fill_between(grid1, np.clip(mean1 - sem1, 0, 1), np.clip(mean1 + sem1, 0, 1), alpha=0.3, color=\"#0073e6\")\n",
    "ax.step(grid2, mean2, where=\"post\", label=\"2-hop training\", color=\"#ff8000\")\n",
    "ax.fill_between(grid2, np.clip(mean2 - sem2, 0, 1), np.clip(mean2 + sem2, 0, 1), alpha=0.3, color=\"#ffb366\")\n",
    "ax.step(grid3, mean3, where=\"post\", label=\"Multi-hop training\", color=\"#9966ff\")\n",
    "ax.fill_between(grid3, np.clip(mean3 - sem3, 0, 1), np.clip(mean3 + sem3, 0, 1), alpha=0.3, color=\"#dabfff\")\n",
    "ax.set_xlabel(\"Message Length\")\n",
    "ax.set_ylabel(\"Empirical CDF\")\n",
    "ax.legend(loc=\"upper center\", bbox_to_anchor=(0.5, -0.15), ncol=3, frameon=False)\n",
    "ax.set_ylim(0, 1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "egg311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
