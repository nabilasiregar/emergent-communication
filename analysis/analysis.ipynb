{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d151dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import pdist\n",
    "from scipy.stats import spearmanr, rankdata, entropy\n",
    "from scipy import stats\n",
    "from typing import List, Dict, Any, Optional\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7722cda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _set_global_seed(seed: int):\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.use_deterministic_algorithms(True)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "_set_global_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a5fcfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_latest_interaction_file(\n",
    "    logs_root: str,\n",
    "    seed_folder: str,\n",
    "    split: str = \"validation\",\n",
    "    prefix: str = \"interaction_gpu0\"\n",
    ") -> object:\n",
    "    pattern = os.path.join(\n",
    "        logs_root,\n",
    "        seed_folder,\n",
    "        \"interactions\",\n",
    "        split,\n",
    "        \"epoch_*\",\n",
    "        f\"{prefix}*\"\n",
    "    )\n",
    "    files = glob.glob(pattern)\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No files matching {pattern}\")\n",
    "\n",
    "    # parse epoch number from folder name\n",
    "    def parse_epoch(path: str) -> int:\n",
    "        folder = os.path.basename(os.path.dirname(path))\n",
    "        return int(folder.split(\"_\", 1)[1])\n",
    "\n",
    "    # sort files by epoch and select the last one\n",
    "    sorted_files = sorted(files, key=parse_epoch)\n",
    "    last_file = sorted_files[-1]\n",
    "    print(last_file)\n",
    "\n",
    "    return torch.load(last_file, map_location=\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2743e59f",
   "metadata": {},
   "source": [
    "# Bee Language Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9a31b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "bee_interaction_obj = load_latest_interaction_file(\n",
    "    logs_root=\"../logs/interactions/2025-07-02\",\n",
    "    seed_folder=\"gamesize10_bee_gs_seed42\",\n",
    "    split=\"validation\",\n",
    "    prefix=\"interaction_gpu0\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2c33bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIRECTIONS = {\n",
    "    \"N\": 0, \"NE\": 1, \"E\": 2, \"SE\": 3,\n",
    "    \"S\": 4, \"SW\": 5, \"W\": 6, \"NW\": 7\n",
    "}\n",
    "# inverse mapping for integer codes back to strings\n",
    "INV_DIRECTIONS: Dict[int, str] = {v: k for k, v in DIRECTIONS.items()}\n",
    "\n",
    "SECTOR_ANGLE = 2 * np.pi / len(DIRECTIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959c59f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph(batch) -> nx.DiGraph:\n",
    "    \"\"\"\n",
    "    Construct a directed graph from a DataBatch\n",
    "    \"\"\"\n",
    "    edge_indices = batch.edge_index.cpu().numpy().T\n",
    "    edge_attrs = batch.edge_attr.cpu().numpy()\n",
    "\n",
    "    graph = nx.DiGraph()\n",
    "    for (u, v), (dist, dir_raw) in zip(edge_indices, edge_attrs):\n",
    "        ui, vi = int(u), int(v)\n",
    "        if isinstance(dir_raw, (bytes, str)):\n",
    "            dir_str = dir_raw.decode() if isinstance(dir_raw, bytes) else dir_raw\n",
    "            if dir_str not in DIRECTIONS:\n",
    "                raise ValueError(f\"Unknown direction '{dir_str}' in edge_attrs\")\n",
    "        else:\n",
    "            dir_int = int(dir_raw)\n",
    "            dir_str = INV_DIRECTIONS.get(dir_int)\n",
    "            if dir_str is None:\n",
    "                raise ValueError(f\"Unknown direction code {dir_int} in edge_attrs\")\n",
    "        graph.add_edge(ui, vi, distance=float(dist), direction=dir_str)\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b930b59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the build graph function on 1 graph\n",
    "first_batch = bee_interaction_obj.aux_input[\"data\"][0]\n",
    "graphs = first_batch.to_data_list()  \n",
    "single = graphs[0]\n",
    "G = build_graph(single)\n",
    "print(f\"Graph has {G.number_of_nodes()} nodes and {G.number_of_edges()} edges\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e4416d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for u, v, attrs in list(G.edges(data=True))[:5]:\n",
    "    print(f\"  {u}->{v} dist={attrs['distance']:.2f}, dir={attrs['direction']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f996dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_single_batch(batch):\n",
    "    G = build_graph(batch)\n",
    "\n",
    "    pos = {i: tuple(batch.pos[i].cpu().numpy()) for i in range(batch.pos.size(0))}\n",
    "\n",
    "    # x[:,0]=nest, x[:,1]=food, x[:,2]=distractor\n",
    "    types = {}\n",
    "    for i, feat in enumerate(batch.x.cpu().numpy()):\n",
    "        idx = feat.argmax()\n",
    "        types[i] = \"nest\" if idx == 0 else \"food\" if idx == 1 else \"distractor\"\n",
    "\n",
    "    color_map = {\"nest\": \"lightblue\", \"food\": \"red\", \"distractor\": \"grey\"}\n",
    "    node_colors = [color_map[types[i]] for i in G.nodes()]\n",
    "    labels = {i: types[i] for i in G.nodes()}\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    nx.draw_networkx_nodes(G, pos, node_color=node_colors, node_size=500)\n",
    "    nx.draw_networkx_labels(G, pos, labels, font_size=8)\n",
    "\n",
    "    for u, v, d in G.edges(data=True):\n",
    "        nx.draw_networkx_edges(\n",
    "            G,\n",
    "            pos,\n",
    "            edgelist=[(u, v)],\n",
    "            arrowstyle='-|>',\n",
    "            arrowsize=12,\n",
    "            connectionstyle='arc3,rad=0.1'\n",
    "        )\n",
    "        edge_label = { (u, v): f\"{d['distance']:.1f}m, dir={d['direction']}\" }\n",
    "        nx.draw_networkx_edge_labels(\n",
    "            G,\n",
    "            pos,\n",
    "            edge_labels=edge_label,\n",
    "            font_size=7,\n",
    "            label_pos=0.4\n",
    "        )\n",
    "\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e8e23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_single_batch(single)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b47780",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_meaning_spaces(\n",
    "    interaction,\n",
    "    hypothesis: str\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Generate meaning vectors [distance, direction] under specified hypotheses.\n",
    "\n",
    "    Always assumes shortest path and uses only edge attributes (except for coordinates).\n",
    "    \"\"\"\n",
    "    # split batched graphs into individual graph\n",
    "    data_list = []\n",
    "    for batch in interaction.aux_input['data']:\n",
    "        data_list.extend(batch.to_data_list())\n",
    "\n",
    "    nest_indices = interaction.aux_input['nest_idx'].tolist()\n",
    "    food_indices = interaction.aux_input['food_idx'].tolist()\n",
    "    if len(data_list) != len(nest_indices):\n",
    "        raise ValueError(\n",
    "            f\"Mismatch examples vs indices: {len(data_list)} graphs but {len(nest_indices)} nest indices\"\n",
    "        )\n",
    "\n",
    "    representations: List[List[float]] = []\n",
    "\n",
    "    \n",
    "\n",
    "    for i, data in enumerate(data_list):\n",
    "        # positions and graph\n",
    "        pos = data.pos.cpu().numpy()\n",
    "        G   = build_graph(data)\n",
    "\n",
    "        # local start/target\n",
    "        start_idx  = nest_indices[i]\n",
    "        target_idx = food_indices[i]\n",
    "        p0, p1     = pos[start_idx], pos[target_idx]\n",
    "\n",
    "        # shortest-path\n",
    "        hop_paths = list(nx.all_shortest_paths(G, source=start_idx, target=target_idx)) \n",
    "        def total_dist(path):\n",
    "            return sum(G[u][v]['distance'] for u, v in zip(path, path[1:]))\n",
    "        path_info = [(p, total_dist(p)) for p in hop_paths]\n",
    "        min_d = min(d for _, d in path_info)\n",
    "        best_paths = [p for p, d in path_info if d == min_d]\n",
    "        best_path = min(best_paths)\n",
    "        edges     = list(zip(best_path, best_path[1:]))\n",
    "\n",
    "        DIRECTION_TO_DEGREES_VOCAB: Dict[str, float] = {\n",
    "        \"N\": 90.0, \"NE\": 45.0, \"E\": 0.0, \"SE\": 315.0,\n",
    "        \"S\": 270.0, \"SW\": 225.0, \"W\": 180.0, \"NW\": 135.0\n",
    "    }\n",
    "        COMPASS_VECS: Dict[str, np.ndarray] = {\n",
    "            d: np.array([np.cos(np.deg2rad(phi)), np.sin(np.deg2rad(phi))])\n",
    "            for d, phi in DIRECTION_TO_DEGREES_VOCAB.items()\n",
    "        }\n",
    "        SECTOR_ANGLE = 2 * np.pi / 8\n",
    "\n",
    "        def degrees_to_discrete_direction(degrees):\n",
    "            \"\"\"Convert degrees to discrete direction using binning\"\"\"\n",
    "            # Normalize to 0-360\n",
    "            degrees = degrees % 360.0\n",
    "            # Bin into 8 sectors (each 45 degrees wide)\n",
    "            sector_idx = int((degrees + 22.5) // 45) % 8\n",
    "            # Map sector index to direction\n",
    "            sectors = [\"E\", \"NE\", \"N\", \"NW\", \"W\", \"SW\", \"S\", \"SE\"]\n",
    "            return sectors[sector_idx]\n",
    "\n",
    "        # prepare edge attributes\n",
    "        edge_dirs = [G[u][v]['direction'] for u, v in edges]\n",
    "        edge_degs = [DIRECTION_TO_DEGREES_VOCAB[dir_str] for dir_str in edge_dirs]\n",
    "        edge_vecs = [COMPASS_VECS[dir_str] for dir_str in edge_dirs]\n",
    "        edge_dists= [G[u][v]['distance'] for u, v in edges]\n",
    "\n",
    "        total_dist = float(sum(edge_dists))\n",
    "        hop_count  = float(len(edges))\n",
    "\n",
    "        # straight-line metrics\n",
    "        dx, dy      = p1[0] - p0[0], p1[1] - p0[1]\n",
    "        straight_dist = float(np.hypot(dx, dy))\n",
    "        straight_ang  = float(np.arctan2(dy, dx))\n",
    "\n",
    "        # angle arithmetic: sum of degrees\n",
    "        # treats directions like rotations (turning left/right)\n",
    "        sum_deg = sum(edge_degs) % 360.0\n",
    "        # map to sector index\n",
    "        sector_idx = int((sum_deg + 22.5) // 45) % 8\n",
    "        angle_arith = sector_idx * SECTOR_ANGLE\n",
    "\n",
    "        # vector sum compass\n",
    "        # treats distance like displacements (moving through space)\n",
    "        vsum = np.sum(edge_vecs, axis=0) if edge_vecs else np.array([0.0, 0.0])\n",
    "        if np.allclose(vsum, 0):\n",
    "            # fallback to first direction\n",
    "            sector_idx_vs = list(DIRECTION_TO_DEGREES_VOCAB.keys()).index(edge_dirs[0]) if edge_dirs else 0\n",
    "        else:\n",
    "            ang_v_deg = np.degrees(np.arctan2(vsum[1], vsum[0])) % 360.0\n",
    "            sector_idx_vs = int((ang_v_deg + 22.5) // 45) % 8\n",
    "        angle_vs = sector_idx_vs * SECTOR_ANGLE\n",
    "\n",
    "        if hypothesis == 'coordinates':\n",
    "            distance, direction = straight_dist, straight_ang\n",
    "\n",
    "        elif hypothesis == 'hop_count_distance_vector_sum_direction':\n",
    "            distance, direction = hop_count, angle_vs\n",
    "\n",
    "        elif hypothesis == 'sum_distances_vector_sum_direction':\n",
    "            distance, direction = total_dist, angle_vs\n",
    "\n",
    "        elif hypothesis == 'hop_count_distance_angle_direction':\n",
    "            distance, direction = hop_count, angle_arith\n",
    "\n",
    "        elif hypothesis == 'sum_distances_angle_direction':\n",
    "            distance, direction = total_dist, angle_arith\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown hypothesis {hypothesis}\")\n",
    "\n",
    "        representations.append([distance, direction])\n",
    "\n",
    "    return np.array(representations, dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce35e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_topsim(\n",
    "    message_vectors: np.ndarray,\n",
    "    meaning_vectors: np.ndarray,\n",
    "    normalize: str = \"linear\"\n",
    ") -> float:\n",
    "    # 1) custom message‐space metric on [dir_token, distance]\n",
    "    def message_metric(u, v):\n",
    "        tok_u, ru = int(u[0]) % 8, u[1]\n",
    "        tok_v, rv = int(v[0]) % 8, v[1]\n",
    "        θu = 2*np.pi * tok_u / 8\n",
    "        θv = 2*np.pi * tok_v / 8\n",
    "\n",
    "        dr = ru - rv\n",
    "        Δθ = abs(θu - θv) % (2*np.pi)\n",
    "        if Δθ > np.pi:\n",
    "            Δθ = 2*np.pi - Δθ\n",
    "        Δθ_norm = Δθ / np.pi\n",
    "\n",
    "        return np.hypot(dr, Δθ_norm)\n",
    "\n",
    "    # 2) message distances\n",
    "    msg_dists = pdist(message_vectors, metric=message_metric)\n",
    "    if np.std(msg_dists) == 0:\n",
    "        return np.nan\n",
    "\n",
    "    # 3) extract raw dist & angle from meanings\n",
    "    d = meaning_vectors[:, 0]\n",
    "    a = meaning_vectors[:, 1]\n",
    "    n = len(d)\n",
    "\n",
    "    # 4) normalize the distance column\n",
    "    if normalize == \"linear\":\n",
    "        if d.max() != d.min():\n",
    "            d_norm = (d - d.min()) / (d.max() - d.min())\n",
    "        else:\n",
    "            d_norm = np.zeros_like(d)\n",
    "    elif normalize == \"log\":\n",
    "        d_log = np.log1p(d)\n",
    "        if d_log.max() != d_log.min():\n",
    "            d_norm = (d_log - d_log.min()) / (d_log.max() - d_log.min())\n",
    "        else:\n",
    "            d_norm = np.zeros_like(d_log)\n",
    "    elif normalize == \"rank\":\n",
    "        ranks = rankdata(d, method=\"average\")\n",
    "        d_norm = (ranks - 1) / (n - 1)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown normalize={normalize!r}\")\n",
    "\n",
    "    # 5) build meaning‐space distances\n",
    "    meaning_dists = []\n",
    "    for i in range(n - 1):\n",
    "        for j in range(i + 1, n):\n",
    "            dd = d_norm[i] - d_norm[j]\n",
    "            Δθ = abs(a[i] - a[j]) % (2 * np.pi)\n",
    "            if Δθ > np.pi:\n",
    "                Δθ = 2*np.pi - Δθ\n",
    "            θ_norm = Δθ / np.pi\n",
    "            meaning_dists.append(np.hypot(dd, θ_norm))\n",
    "\n",
    "    meaning_dists = np.array(meaning_dists)\n",
    "    if np.std(meaning_dists) == 0:\n",
    "        return np.nan\n",
    "\n",
    "    # 6) Spearman‐ρ\n",
    "    return spearmanr(msg_dists, meaning_dists, nan_policy=\"raise\").correlation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22f2562",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mutual_info_score\n",
    "\n",
    "def _percentile_bins(x: np.ndarray, n_bins: int) -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Return digitised array and the bin edges used.\"\"\"\n",
    "    pct  = np.linspace(0, 100, n_bins + 1)\n",
    "    edges = np.percentile(x, pct)\n",
    "    edges[-1] += 1e-12\n",
    "    return np.digitize(x, edges) - 1, edges\n",
    "\n",
    "def _shannon_entropy(ids: np.ndarray) -> float:\n",
    "    vals, cnts = np.unique(ids, return_counts=True)\n",
    "    p = cnts / cnts.sum()\n",
    "    return float(-np.sum(p * np.log(p + 1e-12)))\n",
    "\n",
    "def _angle_to_sector(theta: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"8-way compass bin  (E=0, NE=1, …).\"\"\"\n",
    "    return (((theta % (2*np.pi)) + np.pi/8) // (np.pi/4)).astype(int)\n",
    "\n",
    "def compute_posdis(\n",
    "    meanings : np.ndarray, # (N,2)  [dist , θ(rad)]\n",
    "    messages : np.ndarray, # (N,2)  [dist_tok , dir_tok]\n",
    "    *, n_bins_distance: int = 2\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Positional disentanglement (Chaabouni et al.2020).\n",
    "    Returns a value in [0,1]; higher means cleaner token-concept alignment.\n",
    "    \"\"\"\n",
    "    # discretise concepts\n",
    "    concept_dist_disc, dist_edges = _percentile_bins(meanings[:, 0], n_bins_distance)\n",
    "    concept_dir_disc  = _angle_to_sector(meanings[:, 1])\n",
    "    concepts          = np.column_stack([concept_dist_disc, concept_dir_disc])\n",
    "\n",
    "    pos_scores = []\n",
    "\n",
    "    for pos in (0, 1):\n",
    "        token_raw = messages[:, pos]\n",
    "\n",
    "        # discretise token if continuous (distance position)\n",
    "        if pos == 0:\n",
    "            token_disc, _ = _percentile_bins(token_raw, n_bins_distance)\n",
    "        else:\n",
    "            token_disc   = token_raw.astype(int)\n",
    "\n",
    "        # mutual information with each concept\n",
    "        mi = [mutual_info_score(token_disc, concepts[:, i]) for i in (0, 1)]\n",
    "        mi_sorted = sorted(mi, reverse=True)\n",
    "        top1, top2 = mi_sorted + [0.0] * (2 - len(mi_sorted))   # pad if less than 2\n",
    "\n",
    "        H = _shannon_entropy(token_disc)\n",
    "        score = (top1 - top2) / H if H > 0 else 0.0\n",
    "        pos_scores.append(score)\n",
    "\n",
    "    total = float(np.mean(pos_scores))\n",
    "    breakdown = {'distance_token' : pos_scores[0],\n",
    "                'direction_token': pos_scores[1]}\n",
    "\n",
    "    return total, breakdown\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2877f45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "\n",
    "class _TREModel(nn.Module):\n",
    "    \"\"\"Linear-composition model  s = A φ_d + B φ_dir  (Appendix C of the paper).\"\"\"\n",
    "    def __init__(self, vocab_size: int, emb: int = 32):\n",
    "        super().__init__()\n",
    "        self.dist_emb = nn.Linear(1, emb)        # φ(distance_token)\n",
    "        self.dir_emb  = nn.Embedding(vocab_size, emb)\n",
    "        self.A = nn.Linear(emb, 3, bias=False)   # -> [z_dist , z_cos , z_sin]\n",
    "        self.B = nn.Linear(emb, 3, bias=False)\n",
    "\n",
    "    def forward(self, dist_t: torch.Tensor, dir_t: torch.Tensor) -> torch.Tensor:\n",
    "        z = self.A(self.dist_emb(dist_t)) + self.B(self.dir_emb(dir_t))\n",
    "        return z \n",
    "\n",
    "def _loss_tre(pred: torch.Tensor, target: torch.Tensor, \n",
    "              dist_weight: float = 1.0, dir_weight: float = 1.0) -> torch.Tensor:\n",
    "    \"\"\"MSE on distance + cos/sin of angle with separate weights.\"\"\"\n",
    "    loss_dist = F.mse_loss(pred[:, 0], target[:, 0])\n",
    "    loss_dir  = F.mse_loss(pred[:, 1:], target[:, 1:])\n",
    "    return dist_weight * loss_dist + dir_weight * loss_dir\n",
    "\n",
    "def compute_tre(\n",
    "    meanings : np.ndarray,             # (N,2)   [distance , θ(rad)]\n",
    "    messages : np.ndarray,             # (N,2)   [distance_token , dir_token]\n",
    "    hyperparams: Optional[Dict[str, Any]] = None\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Tree-reconstruction error.  Lower means more compositional.\n",
    "    (We report the best validation loss, like the paper.)\n",
    "    \"\"\"\n",
    "    hp = dict(batch_size=256, val_split=0.2, epochs=300,\n",
    "              lr=1e-2, weight_decay=1e-5, seed=42, emb=32)\n",
    "    if hyperparams: hp.update(hyperparams)\n",
    "    _set_global_seed(hp[\"seed\"])\n",
    "\n",
    "    torch.manual_seed(hp[\"seed\"]);  np.random.seed(hp[\"seed\"])\n",
    "\n",
    "    # ── prepare tensors ────────────────────────────────────────────────────\n",
    "    dist_tok = messages[:, 0].astype(np.float32).reshape(-1, 1)\n",
    "    dist_tok = (dist_tok - dist_tok.mean()) / (dist_tok.std() + 1e-8)   # z‑score\n",
    "    dir_tok  = messages[:, 1].astype(np.int64)\n",
    "    vocab    = int(dir_tok.max()) + 1\n",
    "\n",
    "    # targets: distance  +  angle→(cos,sin)\n",
    "    y_dist = dist_tok                          # same scale as input (z‑scored)\n",
    "    y_vec  = np.column_stack([np.cos(meanings[:, 1]), np.sin(meanings[:, 1])])\n",
    "    y      = np.column_stack([y_dist, y_vec]).astype(np.float32)\n",
    "\n",
    "    # to torch\n",
    "    Xd = torch.from_numpy(dist_tok)\n",
    "    Xc = torch.from_numpy(dir_tok)\n",
    "    Y  = torch.from_numpy(y)\n",
    "\n",
    "    dataset  = TensorDataset(Xd, Xc, Y)\n",
    "    n_val    = int(hp[\"val_split\"] * len(dataset))\n",
    "    n_train  = len(dataset) - n_val\n",
    "\n",
    "    g = torch.Generator()\n",
    "    g.manual_seed(hp[\"seed\"])\n",
    "    ds_train, ds_val = random_split(dataset, [n_train, n_val], generator=g)\n",
    "\n",
    "    ld_train = DataLoader(ds_train, batch_size=hp[\"batch_size\"], shuffle=True, num_workers=0, generator=torch.Generator().manual_seed(hp[\"seed\"]))\n",
    "    ld_val   = DataLoader(ds_val, batch_size=hp[\"batch_size\"], num_workers=0)\n",
    "\n",
    "    model = _TREModel(vocab, emb=hp[\"emb\"])\n",
    "    opt   = torch.optim.Adam(model.parameters(), lr=hp[\"lr\"],\n",
    "                             weight_decay=hp[\"weight_decay\"])\n",
    "\n",
    "    best = np.inf;  patience = 25;  bad = 0\n",
    "    for _ in range(hp[\"epochs\"]):\n",
    "        model.train()\n",
    "        for xd, xc, y in ld_train:\n",
    "            loss = _loss_tre(model(xd, xc), y, hp[\"dist_weight\"], hp[\"dir_weight\"])\n",
    "            opt.zero_grad(); loss.backward(); opt.step()\n",
    "\n",
    "        # validation\n",
    "        model.eval();  val = 0.0\n",
    "        with torch.no_grad():\n",
    "            for xd, xc, y in ld_val:\n",
    "                val += _loss_tre(model(xd, xc), y, hp[\"dist_weight\"], hp[\"dir_weight\"]).item() * len(y)\n",
    "        val /= n_val\n",
    "\n",
    "        if val < best - 1e-4:\n",
    "            best, bad = val, 0\n",
    "        else:\n",
    "            bad += 1\n",
    "            if bad >= patience: break          # early stop\n",
    "\n",
    "    return float(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9bb4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 8\n",
    "token_directions = bee_interaction_obj.message[:, :8].argmax(dim=-1)\n",
    "token_distances = bee_interaction_obj.message[:, -1]\n",
    "messages = torch.stack([token_directions, token_distances], dim=1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb41ca43",
   "metadata": {},
   "outputs": [],
   "source": [
    "meaning = extract_meaning_spaces(bee_interaction_obj, 'hop_count_distance_vector_sum_direction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb353ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "tre_distance_only = compute_tre(meaning, messages, {'dist_weight': 1.0, 'dir_weight': 0.0})\n",
    "tre_direction_only = compute_tre(meaning, messages, {'dist_weight': 0.0, 'dir_weight': 1.0})\n",
    "tre_distance_heavy = compute_tre(meaning, messages, {'dist_weight': 2.0, 'dir_weight': 1.0})\n",
    "tre_direction_heavy = compute_tre(meaning, messages, {'dist_weight': 1.0, 'dir_weight': 2.0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abb8b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "total, per_tok = compute_posdis(meaning, messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe28579c",
   "metadata": {},
   "source": [
    "# Distance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded3590f",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = np.array(token_distances)\n",
    "fig = plt.figure()\n",
    "plt.hist(raw, bins=50)\n",
    "plt.title(\"Histogram of raw token_distances\")\n",
    "plt.xlabel(\"token_distance\")\n",
    "plt.ylabel(\"count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a514ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "expd = np.exp(raw)\n",
    "fig = plt.figure()\n",
    "plt.hist(expd, bins=50)\n",
    "plt.title(\"Histogram of exp(token_distances)\")\n",
    "plt.xlabel(\"exp(token_distance)\")\n",
    "plt.ylabel(\"count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655b6b13",
   "metadata": {},
   "source": [
    "# Compositionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d2e928",
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_root    = \"../logs/interactions/2025-07-02\"\n",
    "seed_folders = [\n",
    "    # # baseline\n",
    "    'gamesize10_bee_gs_seed42',\n",
    "    'gamesize10_bee_gs_seed123', \n",
    "    'gamesize10_bee_gs_seed2025',\n",
    "    'gamesize10_bee_gs_seed31', \n",
    "    'gamesize10_bee_gs_seed27',\n",
    "    # binned distance\n",
    "    # \"binneddistance_bee_gs_seed27\",\n",
    "    # \"binneddistance_bee_gs_seed31\",\n",
    "    # \"binneddistance_bee_gs_seed42\",\n",
    "    # \"binneddistance_bee_gs_seed123\",\n",
    "    # \"binneddistance_bee_gs_seed2025\"\n",
    "]\n",
    "hypotheses = [\n",
    "    \"coordinates\",\n",
    "    \"hop_count_distance_vector_sum_direction\",\n",
    "    \"sum_distances_vector_sum_direction\",\n",
    "    \"hop_count_distance_angle_direction\",\n",
    "    \"sum_distances_angle_direction\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34dabae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"results\", exist_ok=True)\n",
    "rows = []\n",
    "\n",
    "for seed_folder in seed_folders:\n",
    "    interaction = load_latest_interaction_file(logs_root, seed_folder)\n",
    "\n",
    "    # ── extract emerged messages ─────────────────────────────────────────\n",
    "    token_dirs  = interaction.message[:, :8].argmax(dim=-1)\n",
    "    token_dists = interaction.message[:, -1]\n",
    "    emerged_msgs = torch.stack([token_dirs, token_dists], dim=1).numpy()\n",
    "    print(f\"Processing {seed_folder}: {len(emerged_msgs)} messages\")\n",
    "\n",
    "    rng = np.random.RandomState(int(seed_folder.split(\"seed\")[-1]))\n",
    "    seed = int(seed_folder.split(\"seed\")[-1])\n",
    "    print(f\"Seed: {seed}\")\n",
    "\n",
    "    for hyp in hypotheses:\n",
    "        meaning = extract_meaning_spaces(interaction, hyp)\n",
    "\n",
    "        tl = compute_topsim(emerged_msgs, meaning, normalize=\"linear\")\n",
    "        tg = compute_topsim(emerged_msgs, meaning, normalize=\"log\")\n",
    "        tr = compute_topsim(emerged_msgs, meaning, normalize=\"rank\")\n",
    "\n",
    "        ptot, pbd = compute_posdis(emerged_msgs, meaning)\n",
    "        tre       = compute_tre(meaning, emerged_msgs, {'dist_weight': 1.0, 'dir_weight': 1.0})\n",
    "        tre_dist_only    = compute_tre(meaning,    emerged_msgs,\n",
    "                                       {'dist_weight':1.0, 'dir_weight':0.0})\n",
    "        tre_dir_only     = compute_tre(meaning,    emerged_msgs,\n",
    "                                       {'dist_weight':0.0, 'dir_weight':1.0})\n",
    "        tre_dist_heavy   = compute_tre(meaning,    emerged_msgs,\n",
    "                                       {'dist_weight':2.0, 'dir_weight':1.0})\n",
    "        tre_dir_heavy    = compute_tre(meaning,    emerged_msgs,\n",
    "                                       {'dist_weight':1.0, 'dir_weight':2.0})\n",
    "\n",
    "        rows.append({\n",
    "            'seed':               seed_folder,\n",
    "            'hypothesis':         hyp,\n",
    "            'topsim_lin':         tl,\n",
    "            'topsim_log':         tg,\n",
    "            'topsim_rank':        tr,\n",
    "            'posdis_total':       ptot,\n",
    "            'posdis_distance':    pbd['distance_token'],\n",
    "            'posdis_direction':   pbd['direction_token'],\n",
    "            'tre':                tre,\n",
    "            'tre_dist_only':      tre_dist_only,\n",
    "            'tre_dir_only':       tre_dir_only,\n",
    "            'tre_dist_heavy':     tre_dist_heavy,\n",
    "            'tre_dir_heavy':      tre_dir_heavy\n",
    "        })\n",
    "\n",
    "    # ── random sanity check ───────────────────────────────────────────────\n",
    "    N = len(emerged_msgs)\n",
    "    rand_logits = rng.randn(N, 8)\n",
    "    rand_dist   = rng.rand(N, 1) * 10\n",
    "    rand_dir    = rand_logits.argmax(axis=-1).reshape(-1, 1)\n",
    "    rand_msgs   = np.hstack([rand_dist, rand_dir])\n",
    "\n",
    "    rand_true_d = rng.rand(N) * 10\n",
    "    rand_true_a = rng.rand(N) * 2*np.pi - np.pi\n",
    "    rand_mean   = np.vstack([rand_true_d, rand_true_a]).T\n",
    "\n",
    "    r_l   = compute_topsim(rand_msgs, rand_mean, normalize=\"linear\")\n",
    "    r_g   = compute_topsim(rand_msgs, rand_mean, normalize=\"log\")\n",
    "    r_r   = compute_topsim(rand_msgs, rand_mean, normalize=\"rank\")\n",
    "    rp, rpd = compute_posdis(rand_msgs, rand_mean)\n",
    "    r_tre = compute_tre(rand_mean, rand_msgs, {'dist_weight': 1.0, 'dir_weight': 1.0})\n",
    "\n",
    "    rows.append({\n",
    "        'seed':               seed_folder,\n",
    "        'hypothesis':         'random',\n",
    "        'topsim_lin':         r_l,\n",
    "        'topsim_log':         r_g,\n",
    "        'topsim_rank':        r_r,\n",
    "        'posdis_total':       rp,\n",
    "        'posdis_distance':    rpd['distance_token'],\n",
    "        'posdis_direction':   rpd['direction_token'],\n",
    "        'tre':                r_tre\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(rows, columns=[\n",
    "    'seed',\n",
    "    'hypothesis',\n",
    "    'topsim_lin',\n",
    "    'topsim_log',\n",
    "    'topsim_rank',\n",
    "    'posdis_total',\n",
    "    'posdis_distance',\n",
    "    'posdis_direction',\n",
    "    'tre',\n",
    "    'tre_dist_only',\n",
    "    'tre_dir_only',\n",
    "    'tre_dist_heavy',\n",
    "    'tre_dir_heavy'\n",
    "])\n",
    "df.to_csv(\"../results/bee_compositionality.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca7fe6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.to_csv(\"bee_compositionality.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5807c1",
   "metadata": {},
   "source": [
    "# Human Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36dd0b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_interaction_obj = load_latest_interaction_file(\n",
    "    logs_root=\"../logs/interactions/2025-06-22\",\n",
    "    seed_folder=\"maxlen10_human_gs_seed42\",\n",
    "    split=\"validation\",\n",
    "    prefix=\"interaction_gpu0\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea41771",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_interaction_obj.message.argmax(-1)[:, :-1].cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7479a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD = 0\n",
    "DIRECTIONS = { \"N\":0,  \"NE\":1,  \"E\":2,  \"SE\":3,\n",
    "               \"S\":4,  \"SW\":5,  \"W\":6,  \"NW\":7 }\n",
    "\n",
    "def intify(seq, vocab):\n",
    "    \"\"\"Map symbols to ints ≥1; PAD stays 0.\"\"\"\n",
    "    out = []\n",
    "    for s in seq:\n",
    "        if s not in vocab:\n",
    "            vocab[s] = len(vocab) + 1\n",
    "        out.append(vocab[s])\n",
    "    return out\n",
    "\n",
    "def pad_to_max_len(seqs, max_len):\n",
    "    \"\"\"Right-pad or truncate each int sequence to max_len.\"\"\"\n",
    "    M = len(seqs)\n",
    "    out = np.zeros((M, max_len), dtype=int)\n",
    "    for i, s in enumerate(seqs):\n",
    "        ln = min(len(s), max_len)\n",
    "        out[i,:ln] = s[:ln]\n",
    "    return out\n",
    "\n",
    "def extract_truth_sequences(interaction, max_len, dist_bins=3):\n",
    "    \"\"\"\n",
    "    For each example in interaction:\n",
    "      - Enumerate all fewest-hop paths (nx.all_shortest_paths)\n",
    "      - Tie-break by summing each path's G[u][v]['distance'] → pick minimal\n",
    "      - Read G[u][v]['direction'] → 0..7 via DIRECTIONS\n",
    "      - Collect raw distances, bin globally into `dist_bins` percentile bins → 0..dist_bins-1\n",
    "      - Pad/truncate both sequences to max_len (PAD=0)\n",
    "    Returns:\n",
    "      truth_dirs, truth_dists: List[List[int]] each of length max_len\n",
    "    \"\"\"\n",
    "    # 1) gather graphs & indices\n",
    "    data_list, raw_paths = [], []\n",
    "    for batch in interaction.aux_input['data']:\n",
    "        data_list.extend(batch.to_data_list())\n",
    "    nests = interaction.aux_input['nest_idx'].tolist()\n",
    "    foods = interaction.aux_input['food_idx'].tolist()\n",
    "    if len(data_list) != len(nests):\n",
    "        raise ValueError(\"mismatch data vs indices\")\n",
    "\n",
    "    # 2) collect all raw distances for binning later\n",
    "    all_dists = []\n",
    "    for i, data in enumerate(data_list):\n",
    "        G = build_graph(data)\n",
    "        s, t = nests[i], foods[i]\n",
    "        # all fewest‐hop paths\n",
    "        cands = list(nx.all_shortest_paths(G, source=s, target=t))\n",
    "        # pick by minimal sum of edge‐distance\n",
    "        def cost(path):\n",
    "            return sum(G[u][v]['distance'] for u,v in zip(path,path[1:]))\n",
    "        best = min(cands, key=cost)\n",
    "        # record its edges\n",
    "        eds = []\n",
    "        for u,v in zip(best, best[1:]):\n",
    "            raw = G[u][v]['distance']\n",
    "            all_dists.append(raw)\n",
    "            eds.append((G[u][v]['direction'], raw))\n",
    "        raw_paths.append(eds)\n",
    "\n",
    "    # 3) global percentile bins for distances\n",
    "    pct = np.linspace(0,100,dist_bins+1)\n",
    "    edges = np.percentile(all_dists, pct)\n",
    "    edges[-1] += 1e-12  # close last bin\n",
    "\n",
    "    # 4) build discrete sequences\n",
    "    truth_dirs, truth_dists = [], []\n",
    "    for eds in raw_paths:\n",
    "        dirs, dists = [], []\n",
    "        for dstr, raw in eds:\n",
    "            dirs.append(DIRECTIONS[dstr])\n",
    "            cat = int(np.digitize(raw, edges)) - 1\n",
    "            dists.append(cat)\n",
    "        # pad/truncate\n",
    "        if len(dirs) < max_len:\n",
    "            padlen = max_len - len(dirs)\n",
    "            dirs  += [PAD]*padlen\n",
    "            dists += [PAD]*padlen\n",
    "        else:\n",
    "            dirs  = dirs[:max_len]\n",
    "            dists = dists[:max_len]\n",
    "        truth_dirs.append(dirs)\n",
    "        truth_dists.append(dists)\n",
    "\n",
    "    return truth_dirs, truth_dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f39e029",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_posdis_seq(msg_seqs, truth_dirs, truth_dists):\n",
    "    \"\"\"\n",
    "    For each slot p:\n",
    "      MI_dir = I(Msg_p;Dir_p)\n",
    "      MI_dst = I(Msg_p;Dist_p)\n",
    "      posdis_p = (max(MI_dir,MI_dst) - min(...)) / H(Msg_p)\n",
    "    Return (mean_posdis, per_slot_list).\n",
    "    \"\"\"\n",
    "    X = np.array(msg_seqs,     dtype=int)\n",
    "    D = np.array(truth_dirs,   dtype=int)\n",
    "    Z = np.array(truth_dists,  dtype=int)\n",
    "    N, L = X.shape\n",
    "\n",
    "    # entropy per slot\n",
    "    H = np.zeros(L)\n",
    "    for p in range(L):\n",
    "        _, cnt = np.unique(X[:,p], return_counts=True)\n",
    "        probs = cnt / cnt.sum()\n",
    "        H[p] = entropy(probs)\n",
    "\n",
    "    # compute MI per slot\n",
    "    scores = []\n",
    "    for p in range(L):\n",
    "        tok = X[:, p]\n",
    "        mi_dir  = mutual_info_score(tok, D[:, p])\n",
    "        mi_dist = mutual_info_score(tok, Z[:, p])\n",
    "        top1, top2 = max(mi_dir,mi_dist), min(mi_dir,mi_dist)\n",
    "        scores.append((top1 - top2)/H[p] if H[p]>0 else 0.0)\n",
    "\n",
    "    return float(np.mean(scores)), scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8538df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TRESeq(nn.Module):\n",
    "    \"\"\"\n",
    "    Takes a derivation sequence of integers (concatenated dir+dist tokens),\n",
    "    embeds and sums them, then predicts the full message sequence via a single linear head.\n",
    "    \"\"\"\n",
    "    def __init__(self, deriv_vocab_size: int, msg_vocab_size: int, seq_len: int, emb: int = 64):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(deriv_vocab_size, emb, padding_idx=PAD)\n",
    "        self.head = nn.Linear(emb, seq_len * msg_vocab_size, bias=False)\n",
    "        self.seq_len = seq_len\n",
    "        self.msg_vocab_size = msg_vocab_size\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        z = self.emb(x).sum(dim=1)\n",
    "        out = self.head(z)\n",
    "        return out.view(-1, self.seq_len, self.msg_vocab_size)\n",
    "\n",
    "def compute_tre_seq(\n",
    "    deriv_seqs: list[list[int]],\n",
    "    msg_seqs:   list[list[int]],\n",
    "    tre_hp:     dict | None = None\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Train TRESeq to predict `msg_seqs` from `deriv_seqs`.\n",
    "    Returns the best validation cross‐entropy loss (lower ⇒ more compositional).\n",
    "    \n",
    "    deriv_seqs : List of integer lists (the concatenated dir+dist derivation tokens)\n",
    "    msg_seqs   : List of integer lists (the actual message token sequences)\n",
    "    tre_hp     : Optional hyperparam overrides, e.g. {'seed':123, 'epochs':100}\n",
    "    \"\"\"\n",
    "    # 1) default hyperparameters\n",
    "    hp = {\n",
    "        'seed': 42,\n",
    "        'emb': 64,\n",
    "        'batch_size': 256,\n",
    "        'val_split': 0.2,\n",
    "        'epochs': 200,\n",
    "        'lr': 1e-2,\n",
    "        'weight_decay': 1e-5,\n",
    "        'patience': 20,\n",
    "    }\n",
    "    if tre_hp:\n",
    "        hp.update(tre_hp)\n",
    "    _set_global_seed(hp['seed'])\n",
    "\n",
    "    # 2) build vocab sizes\n",
    "    deriv_vocab_size = max(max(seq) for seq in deriv_seqs) + 1\n",
    "    msg_vocab_size   = max(max(seq) for seq in msg_seqs)   + 1\n",
    "    L_deriv = max(len(seq) for seq in deriv_seqs)\n",
    "    L_msg   = max(len(seq) for seq in msg_seqs)\n",
    "\n",
    "    # 3) pad sequences to fixed length\n",
    "    def pad(seqs, L):\n",
    "        arr = np.full((len(seqs), L), PAD, dtype=np.int64)\n",
    "        for i, s in enumerate(seqs):\n",
    "            ln = min(len(s), L)\n",
    "            arr[i, :ln] = s[:ln]\n",
    "        return arr\n",
    "    X_deriv = pad(deriv_seqs, L_deriv)\n",
    "    Y_msg   = pad(msg_seqs,   L_msg)\n",
    "\n",
    "    # 4) create PyTorch dataset & split\n",
    "    X_t = torch.from_numpy(X_deriv)\n",
    "    Y_t = torch.from_numpy(Y_msg)\n",
    "    ds = TensorDataset(X_t, Y_t)\n",
    "    n_val = int(hp['val_split'] * len(ds))\n",
    "    n_tr  = len(ds) - n_val\n",
    "    g = torch.Generator().manual_seed(hp['seed'])\n",
    "    tr_ds, vl_ds = random_split(ds, [n_tr, n_val], generator=g)\n",
    "    tr_ld = DataLoader(tr_ds, batch_size=hp['batch_size'], shuffle=True,\n",
    "                       generator=torch.Generator().manual_seed(hp['seed']))\n",
    "    vl_ld = DataLoader(vl_ds, batch_size=hp['batch_size'])\n",
    "\n",
    "    # 5) model & optimizer\n",
    "    model = TRESeq(deriv_vocab_size, msg_vocab_size, L_msg, emb=hp['emb'])\n",
    "    opt   = torch.optim.Adam(model.parameters(),\n",
    "                             lr=hp['lr'],\n",
    "                             weight_decay=hp['weight_decay'])\n",
    "\n",
    "    # 6) training with early stopping\n",
    "    best_loss, bad = float('inf'), 0\n",
    "    for epoch in range(hp['epochs']):\n",
    "        model.train()\n",
    "        for xb, yb in tr_ld:\n",
    "            logits = model(xb)\n",
    "            loss = F.cross_entropy(\n",
    "                logits.view(-1, msg_vocab_size),\n",
    "                yb.view(-1),\n",
    "                ignore_index=PAD\n",
    "            )\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        total    = 0\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in vl_ld:\n",
    "                logits = model(xb)\n",
    "                l = F.cross_entropy(\n",
    "                    logits.view(-1, msg_vocab_size),\n",
    "                    yb.view(-1),\n",
    "                    ignore_index=PAD\n",
    "                )\n",
    "                val_loss += l.item() * yb.numel()\n",
    "                total    += yb.numel()\n",
    "        val_loss /= total\n",
    "\n",
    "        if val_loss < best_loss - 1e-4:\n",
    "            best_loss, bad = val_loss, 0\n",
    "        else:\n",
    "            bad += 1\n",
    "            if bad >= hp['patience']:\n",
    "                break\n",
    "\n",
    "    return best_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63500fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _ed(a, b):\n",
    "    \"\"\"Levenshtein edit distance on integer tuples.\"\"\"\n",
    "    m,n = len(a), len(b)\n",
    "    dp = list(range(n+1))\n",
    "    for i in range(1,m+1):\n",
    "        prev, dp[0] = dp[0], i\n",
    "        for j in range(1,n+1):\n",
    "            cost = 0 if a[i-1]==b[j-1] else 1\n",
    "            cur  = min(prev+cost, dp[j]+1, dp[j-1]+1)\n",
    "            prev, dp[j] = dp[j], cur\n",
    "    return dp[-1]\n",
    "\n",
    "def compute_topsim_seq(msg_seqs, truth_seqs):\n",
    "    \"\"\"\n",
    "    Spearman‐ρ between:\n",
    "      msg_dist(i,j)   = edit_dist(msg_i,msg_j)/L\n",
    "      truth_dist(i,j) = edit_dist(truth_i,truth_j)/L\n",
    "    Now with nan_policy=\"raise\".\n",
    "    \"\"\"\n",
    "    X = np.array(msg_seqs,   dtype=int)\n",
    "    Y = np.array(truth_seqs, dtype=int)\n",
    "    N, L = X.shape\n",
    "\n",
    "    msg_d = pdist(X, metric=lambda a,b: _ed(tuple(a),tuple(b))/L)\n",
    "    truth_d = pdist(Y, metric=lambda a,b: _ed(tuple(a),tuple(b))/L)\n",
    "\n",
    "    return spearmanr(msg_d, truth_d, nan_policy=\"raise\").correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2937a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "human_seeds = [\n",
    "    \"logs/interactions/2025-07-08/maxlen2_vocab100_human_gs_seed27\",\n",
    "    \"logs/interactions/2025-07-08/maxlen2_vocab100_human_gs_seed31\",\n",
    "    \"logs/interactions/2025-06-22/maxlen2_human_gs_seed42\",\n",
    "    \"logs/interactions/2025-06-23/human_maxlen_sweep_max_len2_seed123\",\n",
    "    \"logs/interactions/2025-06-23/human_maxlen_sweep_max_len2_seed2025\",\n",
    "\n",
    "    \"logs/interactions/2025-07-08/maxlen4_vocab100_human_gs_seed27\",\n",
    "    \"logs/interactions/2025-07-08/maxlen4_vocab100_human_gs_seed31\",\n",
    "    \"logs/interactions/2025-06-22/maxlen4_human_gs_seed42\",\n",
    "    \"logs/interactions/2025-06-23/human_maxlen_sweep_max_len4_seed123\",\n",
    "    \"logs/interactions/2025-06-23/human_maxlen_sweep_max_len4_seed2025\",\n",
    "\n",
    "    \"logs/interactions/2025-07-08/maxlen6_vocab100_human_gs_seed27\",\n",
    "    \"logs/interactions/2025-07-08/maxlen6_vocab100_human_gs_seed31\",\n",
    "    \"logs/interactions/2025-06-22/maxlen6_human_gs_seed42\",\n",
    "    \"logs/interactions/2025-06-23/human_maxlen_sweep_max_len6_seed123\",\n",
    "    \"logs/interactions/2025-06-23/human_maxlen_sweep_max_len6_seed2025\",\n",
    "\n",
    "    \"logs/interactions/2025-07-08/maxlen10_vocab100_human_gs_seed27\",\n",
    "    \"logs/interactions/2025-07-08/maxlen10_vocab100_human_gs_seed31\",\n",
    "    \"logs/interactions/2025-06-22/maxlen10_human_gs_seed42\",\n",
    "    \"logs/interactions/2025-06-23/human_maxlen_sweep_max_len10_seed123\",\n",
    "    \"logs/interactions/2025-06-23/human_maxlen_sweep_max_len10_seed2025\"\n",
    "]\n",
    "\n",
    "logs_root = \"..\"\n",
    "_set_global_seed(42)\n",
    "rows = []\n",
    "for seed_folder in human_seeds:\n",
    "    m = re.search(r\"max[_]?len[_]?(\\d+)\", seed_folder)\n",
    "    if not m:\n",
    "        raise ValueError(f\"Could not parse max_len from '{seed_folder}'\")\n",
    "    max_len = int(m.group(1))\n",
    "\n",
    "    interaction = load_latest_interaction_file(\n",
    "        logs_root=logs_root,\n",
    "        seed_folder=seed_folder,\n",
    "        split='validation',\n",
    "        prefix='interaction_gpu0'\n",
    "    )\n",
    "\n",
    "    truth_dirs, truth_dists = extract_truth_sequences(\n",
    "        interaction,\n",
    "        max_len=max_len,\n",
    "        dist_bins=3\n",
    "    )\n",
    "\n",
    "    msg_ids   = interaction.message.argmax(-1)[:, :-1].cpu().numpy()\n",
    "    msg_seqs  = [ list(row[:max_len]) for row in msg_ids ]\n",
    "\n",
    "    topsim_actual   = compute_topsim_seq(msg_seqs, truth_dirs)\n",
    "    posdis_actual,_ = compute_posdis_seq(msg_seqs, truth_dirs, truth_dists)\n",
    "\n",
    "    truth_dirs_shift  = [[d+1 for d in seq] for seq in truth_dirs]\n",
    "    truth_dists_shift = [[z+1 for z in seq] for seq in truth_dists]\n",
    "\n",
    "    offset = len(DIRECTIONS) \n",
    "    deriv_seqs = []\n",
    "    for dseq, zseq in zip(truth_dirs_shift, truth_dists_shift):\n",
    "        seq = []\n",
    "        for d, z in zip(dseq, zseq):\n",
    "            if d != PAD: \n",
    "                seq.append(d)\n",
    "            if z != PAD:\n",
    "                seq.append(z + offset)\n",
    "        deriv_seqs.append(seq)\n",
    "\n",
    "    tre_actual = compute_tre_seq(deriv_seqs, msg_seqs, tre_hp={'seed':42})\n",
    "\n",
    "    rows.append({\n",
    "        'seed_folder': seed_folder,\n",
    "        'max_len':     max_len,\n",
    "        'type':        'actual',\n",
    "        'TopSim':      topsim_actual,\n",
    "        'PosDis':      posdis_actual,\n",
    "        'TRE':         tre_actual\n",
    "    })\n",
    "\n",
    "    unique_tokens = sorted({t for seq in msg_seqs for t in seq})\n",
    "    np.random.seed(42)\n",
    "    rand_seqs = [\n",
    "        list(np.random.choice(unique_tokens, size=max_len))\n",
    "        for _ in msg_seqs\n",
    "    ]\n",
    "\n",
    "    topsim_rand   = compute_topsim_seq(rand_seqs, truth_dirs)\n",
    "    posdis_rand,_ = compute_posdis_seq(rand_seqs, truth_dirs, truth_dists)\n",
    "    tre_rand      = compute_tre_seq(deriv_seqs, rand_seqs, tre_hp={'seed':42})\n",
    "\n",
    "    rows.append({\n",
    "        'seed_folder': seed_folder,\n",
    "        'max_len':     max_len,\n",
    "        'type':        'random',\n",
    "        'TopSim':      topsim_rand,\n",
    "        'PosDis':      posdis_rand,\n",
    "        'TRE':         tre_rand\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "df.to_csv(\"human_compositionality.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "egg311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
