{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d151dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from language_analysis import load_latest_interaction, load_all_interactions\n",
    "import math\n",
    "import torch\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "from collections import defaultdict, Counter\n",
    "from egg.core.language_analysis import TopographicSimilarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2743e59f",
   "metadata": {},
   "source": [
    "# Bee Language Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9a31b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "interaction_obj = load_latest_interaction(\n",
    "    logs_root=\"../logs/interactions/2025-06-22\",\n",
    "    seed_folder=\"bee_default_seed42\",\n",
    "    split=\"validation\",\n",
    "    prefix=\"interaction_gpu0\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84886976",
   "metadata": {},
   "source": [
    "## Correlation analysis\n",
    "Spearman test: if continuous token ranks higher on samples that truly are farther apart, ρ will be positive even without requiring them to match exactly\n",
    "\n",
    "Accuracy: overall bearing/direction (as if drawing a straight line from nest to food)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd020a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract tokens\n",
    "vocab_size = 8\n",
    "token_directions = interaction_obj.message[:, :8].argmax(dim=-1).tolist()\n",
    "token_distances = interaction_obj.message[:, -1].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd1023e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = interaction_obj.aux_input[\"data\"]\n",
    "if not isinstance(batches, list):\n",
    "    batches = [batches]\n",
    "\n",
    "# get x,y coordinate\n",
    "pos_list = [b.pos for b in batches]\n",
    "all_pos = torch.cat(pos_list, dim=0)\n",
    "\n",
    "# get global node indices\n",
    "nest_tensor = interaction_obj.aux_input[\"nest_tensor\"]\n",
    "food_tensor = interaction_obj.aux_input[\"food_tensor\"]\n",
    "\n",
    "# compute Euclidean distances\n",
    "real_pos_nest = all_pos[nest_tensor]\n",
    "real_pos_food = all_pos[food_tensor]\n",
    "real_distances = (real_pos_food - real_pos_nest).norm(dim=1).tolist()\n",
    "\n",
    "# calculate distance token correlation\n",
    "rho_dist, p_dist = spearmanr(real_distances, token_distances)\n",
    "print(f\"Distance token vs. real displacement: Spearman rho = {rho_dist:.3f} (p={p_dist:.2g})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5d3c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_dirs = []\n",
    "dxdy  = real_pos_food - real_pos_nest\n",
    "for (dx, dy) in dxdy.tolist():\n",
    "    angle = (math.degrees(math.atan2(dy, dx)) + 360) % 360\n",
    "    bin_idx = int((angle + 22.5) // 45) % 8\n",
    "    true_dirs.append(bin_idx)\n",
    "\n",
    "# classification accuracy of direction token\n",
    "correct = sum(td == gd for td, gd in zip(token_directions, true_dirs))\n",
    "acc = correct / len(true_dirs)\n",
    "print(f\"Direction-token accuracy = {acc*100:.1f}%  (random = {100/8:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4f26af",
   "metadata": {},
   "source": [
    "## Find meaning space in bee tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30f41f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Batch\n",
    "\n",
    "aux_input = interaction_obj.aux_input\n",
    "batch_data = aux_input['data']\n",
    "nest_idx = aux_input['nest_idx']\n",
    "food_idx = aux_input['food_idx']\n",
    "messages = interaction_obj.message\n",
    "vocab_size = 8\n",
    "\n",
    "direction_tokens = messages[:, :vocab_size].argmax(dim=-1).tolist()\n",
    "distance_tokens  = messages[:, -1].tolist()\n",
    "\n",
    "all_data = []\n",
    "for b in batch_data:\n",
    "    if isinstance(b, Batch):\n",
    "        all_data.extend(b.to_data_list())\n",
    "    else:\n",
    "        all_data.extend(b)\n",
    "\n",
    "DIRECTIONS = {\"N\":0, \"NE\":1, \"E\":2, \"SE\":3, \"S\":4, \"SW\":5, \"W\":6, \"NW\":7}\n",
    "ID_TO_DIR   = {v:k for k,v in DIRECTIONS.items()}\n",
    "\n",
    "def extract_path_properties(data, nest, food):\n",
    "    G = nx.DiGraph()\n",
    "    edges = data.edge_index.t().tolist()\n",
    "    attrs = data.edge_attr.tolist()\n",
    "    for (u, v), attr in zip(edges, attrs):\n",
    "        w = float(attr[0])\n",
    "        raw = attr[1]\n",
    "        dir_id = DIRECTIONS[raw] if isinstance(raw, str) else int(raw)\n",
    "        G.add_edge(u, v, weight=w, direction=dir_id)\n",
    "        G.add_edge(v, u, weight=w, direction=dir_id)\n",
    "\n",
    "    try:\n",
    "        path = nx.shortest_path(G, source=nest, target=food, weight='weight')\n",
    "    except nx.NetworkXNoPath:\n",
    "        return None\n",
    "    if len(path) < 2:\n",
    "        return None\n",
    "\n",
    "    hop_count = len(path) - 1\n",
    "    total_distance = 0.0\n",
    "    directions = []\n",
    "    distances  = []\n",
    "    for u, v in zip(path, path[1:]):\n",
    "        total_distance += G[u][v]['weight']\n",
    "        distances.append(G[u][v]['weight'])\n",
    "        directions.append(G[u][v]['direction'])\n",
    "\n",
    "    direction_diversity = len(set(directions))\n",
    "    direction_changes   = sum(\n",
    "        1 for i in range(1, len(directions))\n",
    "        if directions[i] != directions[i-1]\n",
    "    )\n",
    "    cnts = Counter(directions)\n",
    "    dom_dir, dom_ct = cnts.most_common(1)[0]\n",
    "    dom_ratio = dom_ct / len(directions)\n",
    "    arr = np.array(distances)\n",
    "\n",
    "    return {\n",
    "        'hop_count': hop_count,\n",
    "        'total_distance': total_distance,\n",
    "        'directions': [ID_TO_DIR[d] for d in directions],\n",
    "        'distances': distances,\n",
    "        'direction_diversity':direction_diversity, # num unique directions in the path\n",
    "        'direction_changes':  direction_changes, # num times the direction changes along the path\n",
    "        'dominant_direction': ID_TO_DIR[dom_dir], # most frequently used direction in the path\n",
    "        'dominant_direction_ratio': dom_ratio, # fraction of the path uses the dominant direction\n",
    "        'avg_edge_distance':  arr.mean(), \n",
    "        'max_edge_distance':  arr.max(), # longest single edge in path\n",
    "        'min_edge_distance':  arr.min(), # shortest single edge in path\n",
    "        'distance_variance':  arr.var(), # how much edge distances vary\n",
    "        'path_complexity':    direction_diversity + direction_changes\n",
    "    }\n",
    "\n",
    "records = []\n",
    "for i, data in enumerate(all_data):\n",
    "    ln = int(nest_idx[i].item())\n",
    "    lf = int(food_idx[i].item())\n",
    "    dt = direction_tokens[i]\n",
    "    rt = distance_tokens[i]\n",
    "    props = extract_path_properties(data, ln, lf)\n",
    "    if props is None:\n",
    "        continue\n",
    "    rec = {\n",
    "        'token_direction': ID_TO_DIR[dt],\n",
    "        'token_distance':  rt\n",
    "    }\n",
    "    rec.update(props)\n",
    "    records.append(rec)\n",
    "\n",
    "df = pd.DataFrame(records)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747c3273",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2558c74",
   "metadata": {},
   "source": [
    "This suggests the distance token encodes perceived navigational effort from nest node to food node rather than raw distance (how hard is this journey?). Now lets validate this if this is true."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656d08e4",
   "metadata": {},
   "source": [
    "### Distance token analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc22706b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "def plot_token_vs_effort(\n",
    "        token_values: np.ndarray,\n",
    "        effort_values: np.ndarray,\n",
    "        axis: plt.Axes,\n",
    "        effort_label: str\n",
    "    ):\n",
    "    # Rank-correlation (monotonic)\n",
    "    spearman_rho, spearman_pval = spearmanr(token_values, effort_values)\n",
    "\n",
    "    # linear model: token ≈ a·effort + b\n",
    "    X_design          = effort_values.reshape(-1, 1)\n",
    "    y_target          = token_values\n",
    "    model             = LinearRegression().fit(X_design, y_target)\n",
    "    token_predicted   = model.predict(X_design)\n",
    "    r2_linear_fit     = r2_score(y_target, token_predicted)\n",
    "\n",
    "    axis.scatter(effort_values, token_values, alpha=0.45, label=\"samples\")\n",
    "    effort_grid = np.linspace(effort_values.min(),\n",
    "                              effort_values.max(), 200)\n",
    "    axis.plot(\n",
    "        effort_grid,\n",
    "        model.predict(effort_grid.reshape(-1, 1)),\n",
    "        color=\"red\", lw=2,\n",
    "        label=f\"fit  a·x + b  (a={model.coef_[0]:.2f})\"\n",
    "    )\n",
    "    axis.set_xlabel(effort_label)\n",
    "    axis.set_ylabel(\"continuous token value\")\n",
    "    axis.set_title(f\"{effort_label}\\nρ={spearman_rho:.3f}, \"\n",
    "                   f\"R²={r2_linear_fit:.3f}\")\n",
    "    axis.legend()\n",
    "\n",
    "    return spearman_rho, r2_linear_fit, spearman_pval\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15,4))\n",
    "\n",
    "ρ1, R1, p1 = plot_token_vs_effort(df['token_distance'].values,\n",
    "                        df['total_distance'].values, axes[0], \"Total distance\")\n",
    "ρ2, R2, p2 = plot_token_vs_effort(df['token_distance'].values,\n",
    "                        df['hop_count'].astype(float).values, axes[1], \"Hop count\")\n",
    "ρ3, R3, p3 = plot_token_vs_effort(df['token_distance'].values,\n",
    "                        df['path_complexity'].astype(float).values, axes[2], \"Path complexity\")\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "print(f\"Spearman ρ (token vs total_distance) = {ρ1:.3f} (p={p1:.2g})\")\n",
    "print(f\"Spearman ρ (token vs hop_count) = {ρ2:.3f} (p={p2:.2g})\")\n",
    "print(f\"Spearman ρ (token vs path_complexity) = {ρ3:.3f} (p={p3:.2g})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de68c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = df.head(5)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15,4))\n",
    "\n",
    "plot_token_vs_effort(subset['token_distance'].values,\n",
    "                     subset['total_distance'].values,\n",
    "                     axes[0], 'Total distance')\n",
    "\n",
    "plot_token_vs_effort(subset['token_distance'].values,\n",
    "                     subset['hop_count'].values.astype(float),\n",
    "                     axes[1], 'Hop count')\n",
    "\n",
    "plot_token_vs_effort(subset['token_distance'].values,\n",
    "                     subset['path_complexity'].values.astype(float),\n",
    "                     axes[2], 'Path complexity')\n",
    "\n",
    "plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d340ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['token_distance'].hist(bins=8, figsize=(5,3))\n",
    "plt.xlabel(\"token_distance value\")\n",
    "plt.ylabel(\"frequency\")\n",
    "plt.title(\"Distribution of continuous token\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd045770",
   "metadata": {},
   "source": [
    "### Direction token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77567b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_bidirectional_digraph(data):\n",
    "    graph = nx.DiGraph()\n",
    "    for (src, dst), attr in zip(data.edge_index.t().tolist(), data.edge_attr.tolist()):\n",
    "        edge_distance, raw_direction = attr\n",
    "        if isinstance(raw_direction, str):\n",
    "            dir_id = DIRECTIONS[raw_direction]\n",
    "        else: \n",
    "            dir_id = int(raw_direction)\n",
    "\n",
    "        # add both directions so graph is bidirectional\n",
    "        for u, v in ((src, dst), (dst, src)):\n",
    "            graph.add_edge(u, v,\n",
    "                           weight   = float(edge_distance),\n",
    "                           direction= dir_id)\n",
    "    return graph\n",
    "\n",
    "def bearing_sector(vec_xy):\n",
    "    labels = [\"N\",\"NE\",\"E\",\"SE\",\"S\",\"SW\",\"W\",\"NW\"]\n",
    "    angle  = (math.degrees(math.atan2(vec_xy[1], vec_xy[0])) + 360) % 360\n",
    "    return labels[int((angle + 22.5)//45) % 8]\n",
    "\n",
    "records = []\n",
    "\n",
    "for i, data in enumerate(all_data):\n",
    "    nest  = int(nest_idx[i]);   food = int(food_idx[i])\n",
    "    token_s = ID_TO_DIR[direction_tokens[i]]\n",
    "    token_c = distance_tokens[i]\n",
    "\n",
    "    digraph      = build_bidirectional_digraph(data)\n",
    "    undirected   = nx.Graph(digraph)\n",
    "\n",
    "    # Shortest path nest→food (weighted)\n",
    "    path_nodes   = nx.shortest_path(digraph, nest, food, weight='weight')\n",
    "    hop_count    = len(path_nodes) - 1\n",
    "    step_dists   = []\n",
    "    step_dirs    = []\n",
    "    total_dist   = 0.0\n",
    "    for u,v in zip(path_nodes, path_nodes[1:]):\n",
    "        edge_attr   = digraph[u][v]\n",
    "        total_dist += edge_attr['weight']\n",
    "        step_dists.append(edge_attr['weight'])\n",
    "        step_dirs .append(edge_attr['direction'])\n",
    "\n",
    "    direction_diversity = len(set(step_dirs))\n",
    "    direction_changes   = sum(d1!=d2 for d1,d2 in zip(step_dirs, step_dirs[1:])) if hop_count>1 else 0\n",
    "    dom_dir, dom_ct     = Counter(step_dirs).most_common(1)[0]\n",
    "    dom_ratio           = dom_ct / hop_count\n",
    "\n",
    "    # Local connectivity features at food\n",
    "    hub_node        = max(undirected.nodes, key=undirected.degree)\n",
    "    try:\n",
    "        hops_to_hub = nx.shortest_path_length(undirected, food, hub_node)\n",
    "    except nx.NetworkXNoPath:\n",
    "        hops_to_hub = np.nan\n",
    "    two_hop_reach = sum(\n",
    "        1 for n in undirected.nodes\n",
    "        if nx.has_path(undirected, food, n)\n",
    "        and nx.shortest_path_length(undirected, food, n) <= 2\n",
    "    )\n",
    "\n",
    "    import community as louvain\n",
    "    partition     = louvain.best_partition(undirected)\n",
    "    food_comm     = partition[food]\n",
    "\n",
    "    # Straight-line bearing nest→food\n",
    "    straight_bearing = bearing_sector((data.pos[food]-data.pos[nest]).cpu().numpy())\n",
    "\n",
    "    records.append({\n",
    "        'token_direction'           : token_s,\n",
    "        'token_distance'            : token_c,\n",
    "        'ground_truth_sector'       : straight_bearing,\n",
    "        'hop_count'                 : hop_count,\n",
    "        'total_distance'            : total_dist,\n",
    "        'directions'                : [ID_TO_DIR[d] for d in step_dirs],\n",
    "        'direction_diversity'       : direction_diversity,\n",
    "        'direction_changes'         : direction_changes,\n",
    "        'dominant_direction'        : ID_TO_DIR[dom_dir],\n",
    "        'dominant_direction_ratio'  : dom_ratio,\n",
    "        'avg_edge_distance'         : np.mean(step_dists),\n",
    "        'max_edge_distance'         : np.max(step_dists),\n",
    "        'min_edge_distance'         : np.min(step_dists),\n",
    "        'distance_variance'         : np.var(step_dists),\n",
    "        'path_complexity'           : direction_diversity + direction_changes,\n",
    "        'hops_to_hub'               : hops_to_hub,\n",
    "        'hub_degree'                : undirected.degree(hub_node),\n",
    "        'two_hop_reach'             : two_hop_reach,\n",
    "        'food_comm'                 : food_comm\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(records)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab032ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2_contingency, spearmanr, kruskal\n",
    "from sklearn.metrics import mutual_info_score, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import community as louvain \n",
    "\n",
    "print(\"### Goal-Location Sector tests\")\n",
    "sector_contingency = pd.crosstab(df[\"token_direction\"], df[\"ground_truth_sector\"])\n",
    "chi2, p_sector, _, _ = chi2_contingency(sector_contingency)\n",
    "print(f\"Chi-square token×sector: χ²={chi2:.1f},  p={p_sector:.3g}\")\n",
    "\n",
    "acc_sector = (df[\"token_direction\"] == df[\"ground_truth_sector\"]).mean()\n",
    "print(f\"Exact sector accuracy: {acc_sector*100:.1f} %\")\n",
    "\n",
    "le_token  = LabelEncoder().fit_transform(df[\"token_direction\"])\n",
    "le_sector = LabelEncoder().fit_transform(df[\"ground_truth_sector\"])\n",
    "mi_sector = mutual_info_score(le_token, le_sector)\n",
    "print(f\"Mutual information: {mi_sector:.3f} bits\")\n",
    "\n",
    "print(\"\\nConfusion matrix (rows=actual sector, cols=token):\")\n",
    "print(pd.crosstab(df[\"ground_truth_sector\"], df[\"token_direction\"]))\n",
    "\n",
    "# 3) ************************************************************\n",
    "#    LANDMARK-BASED (hub) TESTS\n",
    "# ***************************************************************\n",
    "print(\"\\n### Landmark-reference tests\")\n",
    "rho_hub, p_hub = spearmanr(le_token, df[\"hops_to_hub\"], nan_policy=\"omit\")\n",
    "print(f\"ρ(token , hops_to_hub)  = {rho_hub:.3f} (p={p_hub:.3g})\")\n",
    "\n",
    "median_hops = df[\"hops_to_hub\"].median()\n",
    "near_mask, far_mask = df[\"hops_to_hub\"] <= median_hops, df[\"hops_to_hub\"] > median_hops\n",
    "H_stat, p_kw = kruskal(le_token[near_mask], le_token[far_mask])\n",
    "print(f\"Kruskal near vs far hubs: H={H_stat:.2f}, p={p_kw:.3g}\")\n",
    "\n",
    "# 4) ************************************************************\n",
    "#    CONNECTIVITY / TOPOLOGY PROBES\n",
    "# ***************************************************************\n",
    "print(\"\\n### Connectivity / topology probes\")\n",
    "for col in [\"hub_degree\", \"two_hop_reach\", \"direction_diversity\", \"direction_changes\"]:\n",
    "    rho, p_val = spearmanr(le_token, df[col])\n",
    "    print(f\"ρ(token , {col}) = {rho:+.3f}  (p={p_val:.3g})\")\n",
    "\n",
    "# 5) ************************************************************\n",
    "#    COMMUNITY-BASED HYPOTHESIS\n",
    "# ***************************************************************\n",
    "mi_comm = mutual_info_score(le_token, df[\"food_comm\"])\n",
    "print(f\"\\nMutual information token ↔ food-community: {mi_comm:.3f} bits\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3842ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "compass_labels = [\"N\",\"NE\",\"E\",\"SE\",\"S\",\"SW\",\"W\",\"NW\"]\n",
    "def id_to_label(dir_id):          return compass_labels[dir_id]\n",
    "def label_to_id(label):           return compass_labels.index(label)\n",
    "\n",
    "def dominant_label(labels):\n",
    "    return Counter(labels).most_common(1)[0][0]\n",
    "\n",
    "def net_vector_sector(dir_ids):\n",
    "    x = sum(math.cos(id*math.pi/4) for id in dir_ids)\n",
    "    y = sum(math.sin(id*math.pi/4) for id in dir_ids)\n",
    "    return id_to_label(int(((math.degrees(math.atan2(y,x))+360)%360 + 22.5)//45)%8)\n",
    "\n",
    "df['first_step_sector']    = df['directions'].str[0]\n",
    "df['last_step_sector']     = df['directions'].str[-1]\n",
    "df['dominant_step_sector'] = df['directions'].apply(dominant_label)\n",
    "df['net_vector_sector']    = df['directions'].apply(\n",
    "                                lambda lab: net_vector_sector([label_to_id(x) for x in lab]))\n",
    "\n",
    "def sector_eval(sector_col):\n",
    "    le_truth   = LabelEncoder().fit_transform(df[sector_col])\n",
    "    le_token   = LabelEncoder().fit_transform(df['token_direction'])\n",
    "    acc        = (df[sector_col] == df['token_direction']).mean()*100\n",
    "    mi_bits    = mutual_info_score(le_truth, le_token)\n",
    "    print(f\"{sector_col:22s}  acc={acc:4.1f}%   MI={mi_bits:.3f} bits\")\n",
    "\n",
    "print(\"\\n=== Path-based sector hypotheses ===\")\n",
    "for col in ['first_step_sector','last_step_sector',\n",
    "            'dominant_step_sector','net_vector_sector']:\n",
    "    sector_eval(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea41c8c6",
   "metadata": {},
   "source": [
    "## Message Clustering by Referential Nodes\n",
    "Test if certain nodes consistently serve as a reference point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe23450",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_nest_reference(interaction_obj):\n",
    "    \"\"\"\n",
    "    For each sample, treat the nest node as the reference:\n",
    "      - compute the weighted shortest‐path length from nest to food\n",
    "      - extract the first‐edge direction along that path\n",
    "    Then correlate those meanings against the two message tokens.\n",
    "    \"\"\"\n",
    "    aux_input        = interaction_obj.aux_input\n",
    "    batch_data       = aux_input['data']\n",
    "    nest_idx         = aux_input['nest_idx']\n",
    "    food_idx         = aux_input['food_idx']\n",
    "\n",
    "    messages         = interaction_obj.message\n",
    "    vocab_size       = 8\n",
    "    direction_tokens = messages[:, :vocab_size].argmax(dim=-1).tolist()\n",
    "    distance_tokens  = messages[:, -1].tolist()\n",
    "\n",
    "    path_lengths = []\n",
    "    first_dirs   = []\n",
    "\n",
    "    if not isinstance(batch_data, list):\n",
    "        batch_data = [batch_data]\n",
    "\n",
    "    all_data = []\n",
    "    for b in batch_data:\n",
    "        all_data.extend(b.to_data_list())\n",
    "\n",
    "    for i, data in enumerate(all_data):\n",
    "        ln = int(nest_idx[i].item())\n",
    "        lf = int(food_idx[i].item())\n",
    "        dt =            direction_tokens[i]\n",
    "        # build a bidirectional graph\n",
    "        G = nx.DiGraph()\n",
    "        edges = data.edge_index.t().tolist()\n",
    "        attrs = data.edge_attr.tolist()\n",
    "        for (u, v), attr in zip(edges, attrs):\n",
    "            w = float(attr[0])\n",
    "            G.add_edge(u, v, weight=w)\n",
    "            G.add_edge(v, u, weight=w)\n",
    "\n",
    "        # weighted shortest‐path length nest to food\n",
    "        L = nx.shortest_path_length(G, source=ln, target=lf, weight='weight')\n",
    "        path_lengths.append(L)\n",
    "\n",
    "        # first‐edge direction along that same path\n",
    "        path = nx.shortest_path(G, source=ln, target=lf, weight='weight')\n",
    "        u, v  = path[0], path[1]\n",
    "        # find the matching\n",
    "        for (uu, vv), attr in zip(edges, attrs):\n",
    "            if uu==u and vv==v:\n",
    "                first_dirs.append(int(attr[1]))\n",
    "                break\n",
    "\n",
    "    # Continuous‐token vs path‐length monotonicity\n",
    "    rho, pval = spearmanr(distance_tokens, path_lengths)\n",
    "    print(f\"Nest‐ref dist token vs. graph‐path length: Spearman rho = {rho:.3f} (p={pval:.2g})\")\n",
    "\n",
    "    # Discrete‐token vs first‐edge direction accuracy\n",
    "    acc = sum(d==f for d,f in zip(direction_tokens, first_dirs)) / len(first_dirs)\n",
    "    print(f\"Nest‐ref direction token accuracy = {acc*100:.1f}%  (chance=12.5%)\")\n",
    "\n",
    "    return {\n",
    "        'rho_dist': rho,\n",
    "        'rho_pval': pval,\n",
    "        'dir_acc': acc\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c1b5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_nest_reference(interaction_obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5807c1",
   "metadata": {},
   "source": [
    "# Human Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1533df",
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = load_latest_interaction(\n",
    "    logs_root=\"../logs/interactions/2025-06-22\",\n",
    "    seed_folder=\"maxlen10_human_gs_seed42\",\n",
    "    split=\"validation\",\n",
    "    prefix=\"interaction_gpu0\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb734acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data\n",
    "from typing import List\n",
    "\n",
    "def shortest_path_distance(pyg_graph: Data, src: int, dst: int) -> torch.Tensor:\n",
    "    g = nx.Graph()\n",
    "\n",
    "    u_nodes = pyg_graph.edge_index[0].tolist()\n",
    "    v_nodes = pyg_graph.edge_index[1].tolist()\n",
    "    distances = pyg_graph.edge_attr[:, 1].tolist()\n",
    "\n",
    "    for u, v, d in zip(u_nodes, v_nodes, distances):\n",
    "        g.add_edge(u, v, weight=float(d))\n",
    "\n",
    "    path_len = nx.dijkstra_path_length(g, source=src, target=dst, weight=\"weight\")\n",
    "    return torch.tensor([path_len], dtype=torch.float32)\n",
    "\n",
    "\n",
    "def build_meanings_per_episode(\n",
    "    batched_graphs: List[Data],\n",
    "    nest_idx: torch.Tensor,\n",
    "    food_idx: torch.Tensor,\n",
    ") -> torch.Tensor:\n",
    "    meaning_vectors: List[torch.Tensor] = []\n",
    "    global_id = 0 \n",
    "\n",
    "    for batch in batched_graphs: \n",
    "        episodes = batch.to_data_list()\n",
    "        for local_ep, episode in enumerate(episodes):\n",
    "            src = nest_idx[global_id].item()\n",
    "            dst = food_idx[global_id].item() \n",
    "            meaning_vec = shortest_path_distance(episode, src, dst)\n",
    "            meaning_vectors.append(meaning_vec)\n",
    "            global_id += 1\n",
    "\n",
    "    return torch.vstack(meaning_vectors)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ab73f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39b144e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batches  = logs.aux_input[\"data\"]\n",
    "nests    = logs.aux_input[\"nest_idx\"]\n",
    "foods    = logs.aux_input[\"food_idx\"]\n",
    "\n",
    "meanings = build_meanings_per_episode(batches, nests, foods)\n",
    "\n",
    "human_messages = [m.tolist() for m in logs.message.argmax(dim=-1)]\n",
    "\n",
    "topsim = TopographicSimilarity.compute_topsim(\n",
    "    meanings=meanings,\n",
    "    messages=human_messages,\n",
    "    meaning_distance_fn=\"euclidean\",\n",
    "    message_distance_fn=\"edit\"\n",
    ")\n",
    "print(topsim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2ac189",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb76c490",
   "metadata": {},
   "source": [
    "# TopSim score over epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0702be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_logs = load_all_interactions(\n",
    "    logs_root=\"../logs/interactions\",\n",
    "    seed_folder=\"human_gs_seed42\",\n",
    "    split=\"validation\",\n",
    "    prefix=\"interaction_gpu0\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf909aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "topsim_scores_over_epochs = []\n",
    "pos_topsim_scores_over_epochs = []\n",
    "\n",
    "for i, logs in enumerate(all_logs):\n",
    "    print(f\"Processing Epoch {i}...\")\n",
    "\n",
    "    human_messages = logs.message.argmax(dim=-1)\n",
    "    human_messages_list = [msg.tolist() for msg in human_messages]\n",
    "\n",
    "    meanings = torch.stack([\n",
    "        logs.aux_input[\"nest_tensor\"],\n",
    "        logs.aux_input[\"food_tensor\"]\n",
    "    ], dim=1)\n",
    "\n",
    "    meanings_pos = torch.cat([\n",
    "        logs.aux_input['nest_pos'],\n",
    "        logs.aux_input['food_pos']\n",
    "    ], dim=1)\n",
    "\n",
    "    topsim = TopographicSimilarity.compute_topsim(\n",
    "        meanings=meanings,\n",
    "        messages=human_messages_list,\n",
    "        meaning_distance_fn=\"hamming\",\n",
    "        message_distance_fn=\"edit\"\n",
    "    )\n",
    "\n",
    "    pos_topsim = TopographicSimilarity.compute_topsim(\n",
    "        meanings=meanings_pos,\n",
    "        messages=human_messages_list,\n",
    "        meaning_distance_fn=\"euclidean\",\n",
    "        message_distance_fn=\"edit\"\n",
    "    )\n",
    "\n",
    "    topsim_scores_over_epochs.append(topsim)\n",
    "    pos_topsim_scores_over_epochs.append(pos_topsim)\n",
    "\n",
    "print(\"\\nAll epochs processed.\")\n",
    "print(\"Symbolic TopSim scores:\", topsim_scores_over_epochs)\n",
    "print(\"Positional TopSim scores:\", pos_topsim_scores_over_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b9f508",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = range(len(topsim_scores_over_epochs))\n",
    "\n",
    "plt.figure(figsize=(12, 7))\n",
    "\n",
    "plt.plot(epochs, topsim_scores_over_epochs, linestyle='-', label='Symbolic TopSim (Hamming)')\n",
    "plt.plot(epochs, pos_topsim_scores_over_epochs, linestyle='-', label='Positional TopSim (Euclidean)')\n",
    "\n",
    "plt.title('TopSim over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('TopSim Score')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ba33fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import editdistance\n",
    "logs_root = \"../logs/interactions/2025-06-16\"\n",
    "seed_folders = [\n",
    "    \"gamesize10_bee_gs_seed42\",\n",
    "    \"gamesize10_bee_gs_seed123\",\n",
    "    \"gamesize10_bee_gs_seed2025\",\n",
    "]\n",
    "\n",
    "scores = []\n",
    "for seed in seed_folders:\n",
    "    logs = load_latest_interaction(\n",
    "        logs_root=logs_root,\n",
    "        seed_folder=seed,\n",
    "        split=\"validation\",\n",
    "        prefix=\"interaction_gpu0\"\n",
    "    )\n",
    "\n",
    "    messages = logs.message.argmax(dim=-1).cpu().tolist()\n",
    "\n",
    "    nest = np.stack(logs.aux_input[\"nest_pos\"])\n",
    "    food = np.stack(logs.aux_input[\"food_pos\"])\n",
    "    delta = food - nest\n",
    "    dist = np.linalg.norm(delta, axis=1)\n",
    "    theta = np.arctan2(delta[:,1], delta[:,0])\n",
    "    meanings = np.stack([dist, theta], axis=1)\n",
    "\n",
    "    ts = TopographicSimilarity.compute_topsim(\n",
    "        meanings=meanings,\n",
    "        messages=messages,\n",
    "        meaning_distance_fn=\"euclidean\",\n",
    "        message_distance_fn=lambda x, y: editdistance.eval(x, y) / ((len(x) + len(y)) / 2)\n",
    "    )\n",
    "    scores.append(ts)\n",
    "\n",
    "mean_score = np.mean(scores)\n",
    "std_score  = np.std(scores)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(['Human'], [mean_score], yerr=[std_score], capsize=5)\n",
    "ax.set_ylabel('TopSim Score')\n",
    "ax.set_title('Mean TopSim across seeds')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5904e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import editdistance\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "def compute_cbm(messages, dist_bins, dir_bins, n_dist_bins, n_dir_bins):\n",
    "    C = n_dist_bins + n_dir_bins\n",
    "    Q = defaultdict(int)\n",
    "    for i, msg in enumerate(messages):\n",
    "        # map dir concepts to IDs n_dist_bins…n_dist_bins+n_dir_bins-1\n",
    "        concepts = [ dist_bins[i], n_dist_bins + dir_bins[i] ]\n",
    "        for w in msg:\n",
    "            for v in concepts:\n",
    "                Q[(w,v)] += 1\n",
    "\n",
    "    W = max(w for msg in messages for w in msg) + 1\n",
    "    mat = np.zeros((W, C), int)\n",
    "    for (w,v), cnt in Q.items():\n",
    "        mat[w,v] = cnt\n",
    "\n",
    "    row, col = linear_sum_assignment(-mat)\n",
    "    match_mass = mat[row, col].sum()\n",
    "    norm = sum(max(len(msg), 2) for msg in messages)\n",
    "    return match_mass / norm\n",
    "\n",
    "dx = food[:,0] - nest[:,0]\n",
    "dy = food[:,1] - nest[:,1]\n",
    "distances = np.hypot(dx, dy)  \n",
    "dirs = [\"N\",\"NE\",\"E\",\"SE\",\"S\",\"SW\",\"W\",\"NW\"]\n",
    "angles = (np.degrees(np.arctan2(dy, dx)) % 360)\n",
    "dir_bins = np.array([\n",
    "    int((angle + 22.5)//45) % 8\n",
    "    for angle in angles\n",
    "])\n",
    "n_dist_bins = 1\n",
    "quantiles = np.linspace(0, 1, n_dist_bins+1)\n",
    "edges = np.quantile(distances, quantiles)\n",
    "dist_bins = np.digitize(distances, edges[1:-1])\n",
    "\n",
    "cbm = compute_cbm(\n",
    "    messages=human_messages,\n",
    "    dist_bins=dist_bins,\n",
    "    dir_bins=dir_bins,\n",
    "    n_dist_bins = n_dist_bins,\n",
    "    n_dir_bins  = 8\n",
    ")\n",
    "print(f\"Concept‐Best‐Matching score = {cbm:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "egg311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
