{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0bd5184b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter, defaultdict\n",
    "from egg.core.interaction import Interaction\n",
    "from egg.core.language_analysis import calc_entropy, TopographicSimilarity\n",
    "from sklearn.metrics import mutual_info_score\n",
    "from torch import nn\n",
    "from typing import Dict, Tuple, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5cb2ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CWD is now: /Users/nabilasiregar/code/emergent-communication\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"..\")\n",
    "print(\"CWD is now:\", os.getcwd()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "024befca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found interaction files:\n",
      " → logs/msgs/experiment_bee_with_leakage_10nodes/interactions/validation/epoch_10/run2\n",
      " → logs/msgs/experiment_bee_with_leakage_10nodes/interactions/validation/epoch_10/run3\n",
      " → logs/msgs/experiment_bee_with_leakage_10nodes/interactions/validation/epoch_10/run1\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "candidates = list(Path(\"logs/msgs/experiment_bee_with_leakage_10nodes\").rglob(\"interactions/**/run*\"))\n",
    "print(\"Found interaction files:\")\n",
    "for p in candidates:\n",
    "    print(\" →\", p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d6c1fa49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sequences(interaction_path):\n",
    "    interaction = torch.load(interaction_path, map_location=\"cpu\")\n",
    "    msgs  = interaction.message.cpu().numpy()\n",
    "\n",
    "    tok0 = msgs[:, 0].astype(int)\n",
    "    tok1 = msgs[:, 1] \n",
    "    return tok0, tok1, interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "59aeaac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "bee_path   = \"logs/msgs/experiment_bee_with_leakage_10nodes/interactions/validation/epoch_10/run1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7943a164",
   "metadata": {},
   "outputs": [],
   "source": [
    "direction_token,  distance_token,  bee_int  = load_sequences(bee_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7576d5",
   "metadata": {},
   "source": [
    "1. Mutual Information measures the statistical dependence between two random variables. I use it to check that the direction token carries no information about the distance concept (and vice versa), meaning they are independent. (Shanon et al, 1948)\n",
    "2. Even if each token slot is independent overall, I also want each symbol in a slot to consistently map to the same concept, regardless of what the other slot emits. CI formalizes this: for each concept c, find the symbol s that most often signals c, and measure how exclusively s points back to c. High CI means that symbol meanings dont drift depending on the other token. This alignment-based metric was introduced for emergent communication by Bogin et al. (2018) under the name context-independence\n",
    "3. A compositional protocol should at least allow the reconstruction of a joint message embedding by directly composing the embeddings of its parts (Andreas, J. Measuring Compositionality in Representation Learning. 2019) and (Korbak, T., Zubek, J., & Rączaszek-Leonardi, J. Measuring Non-Trivial Compositionality in Emergent Communication)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4bc7ef59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MI(direction;distance)   = 0.0000\n",
      "MI(distance;direction)   = 0.0000\n",
      "CI_dir→dist (slot1|slot0)= 0.4616\n",
      "CI_dist→dir (slot0|slot1)= 0.9216\n",
      "TRE                       = 0.0000\n"
     ]
    }
   ],
   "source": [
    "tok0 = np.array(direction_token, dtype=int) \n",
    "tok1 = np.array(distance_token, dtype=float)\n",
    "\n",
    "# MI\n",
    "# discretize distance for MI\n",
    "num_bins = 10\n",
    "bins = np.quantile(tok1, np.linspace(0,1,num_bins+1))\n",
    "tok1_disc = np.digitize(tok1, bins[1:-1])\n",
    "mi_0_1 = mutual_info_score(tok0, tok1_disc)\n",
    "mi_1_0 = mutual_info_score(tok1_disc, tok0)\n",
    "\n",
    "# CI\n",
    "def ci_score(sym_arr, other_arr):\n",
    "    N = len(sym_arr)\n",
    "    p_sc = defaultdict(float)\n",
    "    p_cs = defaultdict(float)\n",
    "    for s,o in zip(sym_arr, other_arr):\n",
    "        p_sc[(s,o)] += 1\n",
    "        p_cs[(o,s)] += 1\n",
    "    for k in p_sc: p_sc[k] /= N\n",
    "    for k in p_cs: p_cs[k] /= N\n",
    "\n",
    "    total = 0.0\n",
    "    for o in np.unique(other_arr):\n",
    "        best_symbol = max((s for (oo,s) in p_cs if oo==o), key=lambda s: p_cs[(o,s)])\n",
    "        total += p_cs[(o,best_symbol)] * p_sc.get((best_symbol,o), 0)\n",
    "    return total / len(np.unique(other_arr))\n",
    "\n",
    "ci_dir  = ci_score(tok0, tok1_disc)\n",
    "ci_dist = ci_score(tok1_disc, tok0)\n",
    "\n",
    "def tre_probe(tok0, tok1, emb_dim=16, steps=300, lr=5e-2):\n",
    "    V = int(max(tok0.max(), tok1_disc.max())) + 1\n",
    "    E = nn.Embedding(V, emb_dim)\n",
    "    g = nn.Sequential(nn.Linear(2*emb_dim, emb_dim), nn.Tanh())\n",
    "    opt = torch.optim.Adam(list(E.parameters())+list(g.parameters()), lr=lr)\n",
    "\n",
    "    x0 = torch.tensor(tok0)\n",
    "    x1 = torch.tensor(tok1_disc)\n",
    "    for _ in range(steps):\n",
    "        h_msg = E(x0) + E(x1)\n",
    "        h_hat = g(torch.cat([E(x0), E(x1)], dim=-1))\n",
    "        loss = ((h_msg - h_hat)**2).mean()\n",
    "        opt.zero_grad(); loss.backward(); opt.step()\n",
    "    return float(loss)\n",
    "\n",
    "tre = tre_probe(tok0, tok1)\n",
    "\n",
    "print(f\"MI(direction;distance)   = {mi_0_1:.4f}\")\n",
    "print(f\"MI(distance;direction)   = {mi_1_0:.4f}\")\n",
    "print(f\"CI_dir→dist (slot1|slot0)= {ci_dir:.4f}\")\n",
    "print(f\"CI_dist→dir (slot0|slot1)= {ci_dist:.4f}\")\n",
    "print(f\"TRE                       = {tre:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b847e407",
   "metadata": {},
   "source": [
    "from MI, the two tokens are independent from each other.A CI of 0.46 means that direction symbols are not consistent signallers of any one distance.A CI of 0.92 means that each distance symbol almost always appears with the same direction token."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "egg311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
